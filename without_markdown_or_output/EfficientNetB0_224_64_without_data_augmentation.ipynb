{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import necessary libraries at the top\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration & Constants ---\n",
    "MODEL_NAME = 'EfficientNetB0_Binary'\n",
    "\n",
    "BASE_DATASET_PATH = './k_CBIS-DDSM/'\n",
    "CALC_METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'calc_case(with_jpg_img).csv')\n",
    "MASS_METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'mass_case(with_jpg_img).csv')\n",
    "\n",
    "IMAGE_ROOT_DIR = BASE_DATASET_PATH\n",
    "ACTUAL_IMAGE_FILES_BASE_DIR = os.path.join(IMAGE_ROOT_DIR, 'jpg_img')\n",
    "\n",
    "# Column in CSV that conceptually should point to ROIs, even if paths are flawed\n",
    "CONCEPTUAL_ROI_COLUMN_NAME = 'jpg_ROI_img_path'\n",
    "PATHOLOGY_COLUMN_NAME = 'pathology'\n",
    "CASE_TYPE_COLUMN_NAME = 'case_type'\n",
    "\n",
    "# Model & Training Parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 1\n",
    "FINE_TUNE_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "PATIENCE_EARLY_STOPPING = 25\n",
    "PATIENCE_REDUCE_LR = 10\n",
    "\n",
    "PATIENCE_EARLY_STOPPING_FT = 20\n",
    "PATIENCE_REDUCE_LR_FT = 10\n",
    "\n",
    "OUTPUT_DIR = os.path.join('./', f\"run_{MODEL_NAME}_{IMG_WIDTH}_{BATCH_SIZE}_without_data_augmentation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"All output will be saved to: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "# --- End of Configuration & Constants ---\n",
    "\n",
    "# %%\n",
    "# --- [The data loading and path finding sections remain the same as your original script] ---\n",
    "# ... (Assuming this part runs successfully as in your script)\n",
    "print(\"--- Initial Path Configuration Debug ---\")\n",
    "print(f\"Current working directory (CWD): {os.getcwd()}\")\n",
    "print(f\"BASE_DATASET_PATH (relative from CWD as defined): {BASE_DATASET_PATH}\")\n",
    "print(f\"CALC_METADATA_CSV_PATH (relative from CWD as defined): {CALC_METADATA_CSV_PATH}\")\n",
    "print(f\"MASS_METADATA_CSV_PATH (relative from CWD as defined): {MASS_METADATA_CSV_PATH}\")   # ADDED\n",
    "print(f\"IMAGE_ROOT_DIR (relative from CWD as defined): {IMAGE_ROOT_DIR}\")\n",
    "print(f\"ACTUAL_IMAGE_FILES_BASE_DIR (relative from CWD as defined): {ACTUAL_IMAGE_FILES_BASE_DIR}\")\n",
    "\n",
    "# Resolve to absolute paths for clarity and checking\n",
    "abs_base_dataset_path = os.path.abspath(BASE_DATASET_PATH)\n",
    "abs_calc_metadata_csv_path = os.path.abspath(CALC_METADATA_CSV_PATH)\n",
    "abs_mass_metadata_csv_path = os.path.abspath(MASS_METADATA_CSV_PATH)   # ADDED\n",
    "abs_image_root_dir = os.path.abspath(IMAGE_ROOT_DIR)\n",
    "abs_actual_image_files_base_dir = os.path.abspath(ACTUAL_IMAGE_FILES_BASE_DIR)\n",
    "\n",
    "print(f\"\\nResolved BASE_DATASET_PATH to absolute: {abs_base_dataset_path}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_base_dataset_path)} | Is Dir? {os.path.isdir(abs_base_dataset_path)}\")\n",
    "\n",
    "print(f\"Resolved CALC_METADATA_CSV_PATH to absolute: {abs_calc_metadata_csv_path}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_calc_metadata_csv_path)} | Is File? {os.path.isfile(abs_calc_metadata_csv_path)}\")\n",
    "\n",
    "print(f\"Resolved MASS_METADATA_CSV_PATH to absolute: {abs_mass_metadata_csv_path}\")   # ADDED\n",
    "print(f\"  -> Exists? {os.path.exists(abs_mass_metadata_csv_path)} | Is File? {os.path.isfile(abs_mass_metadata_csv_path)}\")\n",
    "\n",
    "print(f\"Resolved IMAGE_ROOT_DIR to absolute: {abs_image_root_dir}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_image_root_dir)} | Is Dir? {os.path.isdir(abs_image_root_dir)}\")\n",
    "\n",
    "print(f\"Resolved ACTUAL_IMAGE_FILES_BASE_DIR (where series folders should be): {abs_actual_image_files_base_dir}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_actual_image_files_base_dir)} | Is Dir? {os.path.isdir(abs_actual_image_files_base_dir)}\")\n",
    "\n",
    "if os.path.exists(abs_actual_image_files_base_dir) and os.path.isdir(abs_actual_image_files_base_dir):\n",
    "    print(f\"\\nSample contents of ACTUAL_IMAGE_FILES_BASE_DIR ('{abs_actual_image_files_base_dir}') (first 10 items):\")\n",
    "    try:\n",
    "        sample_contents = os.listdir(abs_actual_image_files_base_dir)[:10]\n",
    "        if not sample_contents:\n",
    "            print(\"    -> Directory is empty or unreadable.\")\n",
    "        for item_idx, item in enumerate(sample_contents):\n",
    "            item_abs_path = os.path.join(abs_actual_image_files_base_dir, item)\n",
    "            item_type = \"Dir\" if os.path.isdir(item_abs_path) else \"File\" if os.path.isfile(item_abs_path) else \"Other\"\n",
    "            print(f\"    -> [{item_type}] {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Could not list directory contents: {e}\")\n",
    "else:\n",
    "    print(\"\\nCRITICAL WARNING: ACTUAL_IMAGE_FILES_BASE_DIR does not exist or is not a directory. Path searches will fail.\")\n",
    "print(\"--- End of Initial Path Configuration Debug ---\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Proceeding with CSV loading...\")\n",
    "loaded_dfs = []\n",
    "\n",
    "# Load Calc cases\n",
    "if os.path.exists(abs_calc_metadata_csv_path):\n",
    "    try:\n",
    "        calc_df = pd.read_csv(abs_calc_metadata_csv_path)\n",
    "        calc_df[CASE_TYPE_COLUMN_NAME] = 'calc' # Add case type identifier\n",
    "        loaded_dfs.append(calc_df)\n",
    "        print(f\"Successfully loaded and tagged {len(calc_df)} rows from {CALC_METADATA_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the CALC CSV ({CALC_METADATA_CSV_PATH}): {e}\")\n",
    "else:\n",
    "    print(f\"WARNING: CALC CSV file not found at {abs_calc_metadata_csv_path}. Skipping.\")\n",
    "\n",
    "# Load Mass cases\n",
    "if os.path.exists(abs_mass_metadata_csv_path):\n",
    "    try:\n",
    "        mass_df = pd.read_csv(abs_mass_metadata_csv_path)\n",
    "        mass_df[CASE_TYPE_COLUMN_NAME] = 'mass' # Add case type identifier\n",
    "        loaded_dfs.append(mass_df)\n",
    "        print(f\"Successfully loaded and tagged {len(mass_df)} rows from {MASS_METADATA_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the MASS CSV ({MASS_METADATA_CSV_PATH}): {e}\")\n",
    "else:\n",
    "    print(f\"WARNING: MASS CSV file not found at {abs_mass_metadata_csv_path}. Skipping.\")\n",
    "\n",
    "if not loaded_dfs:\n",
    "    print(\"ERROR: No CSV files were loaded. Cannot proceed.\")\n",
    "    raise FileNotFoundError(\"Neither Calc nor Mass CSV files could be loaded. Check paths and file existence.\")\n",
    "\n",
    "source_df = pd.concat(loaded_dfs, ignore_index=True)\n",
    "print(f\"Combined DataFrame created with {len(source_df)} total rows from {len(loaded_dfs)} CSV file(s).\")\n",
    "print(f\"Columns available in combined DataFrame: {source_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# Clean and filter initial dataframe\n",
    "if CONCEPTUAL_ROI_COLUMN_NAME not in source_df.columns or PATHOLOGY_COLUMN_NAME not in source_df.columns:\n",
    "    print(f\"ERROR: Required columns for metadata ('{CONCEPTUAL_ROI_COLUMN_NAME}' or '{PATHOLOGY_COLUMN_NAME}') not in combined CSV.\")\n",
    "    print(f\"Available columns are: {source_df.columns.tolist()}\")\n",
    "    raise KeyError(\"Missing essential columns in combined CSV.\")\n",
    "\n",
    "source_df.dropna(subset=[CONCEPTUAL_ROI_COLUMN_NAME, PATHOLOGY_COLUMN_NAME], inplace=True)\n",
    "source_df = source_df[source_df[PATHOLOGY_COLUMN_NAME].isin(['MALIGNANT', 'BENIGN'])]\n",
    "print(f\"Rows after initial cleaning (dropna on conceptual ROI/pathology, pathology filter): {len(source_df)}\")\n",
    "\n",
    "if source_df.empty:\n",
    "    raise ValueError(\"Combined DataFrame is empty after initial cleaning. Cannot proceed.\")\n",
    "\n",
    "def heuristic_find_image_path(row, actual_images_root_dir_abs):\n",
    "    try:\n",
    "        patient_id = row['patient_id']\n",
    "        breast_side = row['left or right breast']\n",
    "        image_view = row['image view']\n",
    "        abnormality_id = str(row['abnormality id']) # Ensure it's a string for concatenation\n",
    "\n",
    "        csv_conceptual_roi_path = str(row.get(CONCEPTUAL_ROI_COLUMN_NAME, \"\")).strip()\n",
    "\n",
    "        case_type_folder_prefix = \"\"\n",
    "        if csv_conceptual_roi_path.startswith(\"jpg_img/\"):\n",
    "            path_part = csv_conceptual_roi_path.split('/')[1] # e.g., \"Calc_Training_P_00005_...\" or \"Mass_Test_P_00001_...\"\n",
    "            # Extract the part before patient_id\n",
    "            # The heuristic already includes Mass_Training and Mass_Test\n",
    "            if path_part.startswith(\"Calc_Training_\"): case_type_folder_prefix = \"Calc_Training\"\n",
    "            elif path_part.startswith(\"Calc_Test_\"): case_type_folder_prefix = \"Calc_Test\"\n",
    "            elif path_part.startswith(\"Mass_Training_\"): case_type_folder_prefix = \"Mass_Training\"\n",
    "            elif path_part.startswith(\"Mass_Test_\"): case_type_folder_prefix = \"Mass_Test\"\n",
    "\n",
    "        if not case_type_folder_prefix:\n",
    "            # print(f\"DEBUG (heuristic): Could not determine case_type_folder_prefix for {patient_id} from '{csv_conceptual_roi_path}'\")\n",
    "            return None\n",
    "\n",
    "        # Form search pattern for directories: e.g., /path/to/jpg_img/Calc_Training_P_00005_RIGHT_CC_1-*\n",
    "        dir_search_prefix = f\"{case_type_folder_prefix}_{patient_id}_{breast_side}_{image_view}_{abnormality_id}\"\n",
    "        full_dir_search_pattern = os.path.join(actual_images_root_dir_abs, f\"{dir_search_prefix}-*\")\n",
    "\n",
    "        potential_series_dirs = glob.glob(full_dir_search_pattern)\n",
    "\n",
    "        if not potential_series_dirs:\n",
    "            # print(f\"DEBUG (heuristic): No series directory found for {patient_id} with pattern '{full_dir_search_pattern}'\")\n",
    "            return None\n",
    "\n",
    "        roi_filename_patterns = [\n",
    "            \"ROI-mask-images-img_0-*.jpg\", \"ROI-mask-images-img_1-*.jpg\", \"ROI-mask-images-img_*-*.jpg\"\n",
    "        ]\n",
    "\n",
    "        for series_dir_on_disk in sorted(potential_series_dirs): # Sort to get a consistent choice if multiple match\n",
    "            if os.path.isdir(series_dir_on_disk):\n",
    "                for pattern in roi_filename_patterns:\n",
    "                    image_search_glob = os.path.join(series_dir_on_disk, pattern)\n",
    "                    found_roi_files = glob.glob(image_search_glob)\n",
    "                    if found_roi_files:\n",
    "                        found_roi_files.sort() # Sort to get a consistent choice\n",
    "                        return found_roi_files[0] # Return the first valid ROI found\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # print(f\"DEBUG (heuristic): Error for row {row.get('patient_id', 'Unknown')} ({row.get(CASE_TYPE_COLUMN_NAME, 'N/A')} case): {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Attempting HEURISTIC search for valid ROI paths for each CSV entry...\")\n",
    "source_df['full_image_path'] = source_df.apply(\n",
    "    lambda r: heuristic_find_image_path(r, abs_actual_image_files_base_dir), axis=1\n",
    ")\n",
    "\n",
    "# All columns from source_df (including 'case_type' and any other original metadata)\n",
    "# will be carried into metadata_df for rows where an image path was found.\n",
    "metadata_df = source_df.dropna(subset=['full_image_path']).copy()\n",
    "found_image_count = len(metadata_df)\n",
    "print(f\"Found {found_image_count} actual image files (ROIs if available) after HEURISTIC search from combined data.\")\n",
    "print(f\"Breakdown by case type (if available in metadata_df): \\n{metadata_df[CASE_TYPE_COLUMN_NAME].value_counts()}\")\n",
    "\n",
    "\n",
    "if found_image_count == 0:\n",
    "    print(\"CRITICAL ERROR: Still no valid image files found even after heuristic search from combined data.\")\n",
    "    raise FileNotFoundError(\"No usable image files found even with heuristic search from combined data.\")\n",
    "\n",
    "metadata_df.rename(columns={'full_image_path': 'full_roi_path'}, inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# Ensure 'pathology_encoded' is created correctly on the copied DataFrame slice\n",
    "metadata_df.loc[:, 'pathology_encoded'] = label_encoder.fit_transform(metadata_df[PATHOLOGY_COLUMN_NAME])\n",
    "target_names = list(label_encoder.classes_)\n",
    "\n",
    "# X will contain the image paths, y will contain the encoded labels.\n",
    "# All other metadata columns (like 'patient_id', 'case_type', etc.) remain in metadata_df\n",
    "# and can be used for deeper analysis or if a multi-input model is developed later.\n",
    "X = metadata_df['full_roi_path']\n",
    "y = metadata_df['pathology_encoded']\n",
    "print(f\"Number of samples going into train_test_split: {len(X)}\")\n",
    "\n",
    "if len(X) == 0:\n",
    "     raise ValueError(\"Dataset is empty, cannot split.\")\n",
    "\n",
    "# Stratify by y to ensure balanced splits, especially important if classes are imbalanced\n",
    "# or if combining datasets leads to different proportions.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, random_state=RANDOM_STATE, stratify=y_train_val # Stratify this split too\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Print class distribution in each set to verify stratification\n",
    "print(f\"Train labels distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Validation labels distribution: {np.bincount(y_val)}\")\n",
    "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "\n",
    "def load_image(image_path_tensor, label_tensor):\n",
    "    image_path_str = image_path_tensor.numpy().decode('utf-8')\n",
    "    try:\n",
    "        img = cv2.imread(image_path_str)\n",
    "        if img is None: # Check if image loading failed\n",
    "            dummy_img = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
    "            error_label = np.int32(-1) # Special label to indicate a problem\n",
    "            return dummy_img, error_label\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        return img, label_tensor.numpy().astype(np.int32)\n",
    "    except Exception as e:\n",
    "        dummy_img = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
    "        error_label = np.int32(-1)\n",
    "        return dummy_img, error_label\n",
    "\n",
    "# Data Augmentation has been removed.\n",
    "\n",
    "def create_tf_dataset(image_paths, labels, batch_size):\n",
    "    image_paths_list = list(image_paths)\n",
    "    labels_list = list(labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths_list, labels_list))\n",
    "\n",
    "    # Step 1: Load images\n",
    "    dataset = dataset.map(lambda x, y: tf.py_function(\n",
    "        load_image,\n",
    "        [x, y],\n",
    "        [tf.uint8, tf.int32]),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Step 2: Filter failed loads\n",
    "    dataset = dataset.filter(lambda img, label: label != -1)\n",
    "\n",
    "    # Step 3: Set tensor shapes\n",
    "    def set_shape(img, label):\n",
    "        img.set_shape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "        label.set_shape(())\n",
    "        return img, label\n",
    "    dataset = dataset.map(set_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    # Cast to float32\n",
    "    dataset = dataset.map(lambda img, label: (tf.cast(img, tf.float32), label),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.map(lambda img, label: (img, tf.expand_dims(label, axis=-1)),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Step 4: Apply model-specific preprocessing\n",
    "    dataset = dataset.map(lambda x, y: (tf.keras.applications.efficientnet.preprocess_input(x), y),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Step 5: Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# --- Visualization of augmentation has been removed ---\n",
    "\n",
    "print(\"Recreating TensorFlow datasets with updated image loading logic...\")\n",
    "train_dataset = create_tf_dataset(X_train, y_train, BATCH_SIZE)\n",
    "val_dataset = create_tf_dataset(X_val, y_val, BATCH_SIZE)\n",
    "test_dataset = create_tf_dataset(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "print(\"Verifying dataset integrity (this might take a moment)...\")\n",
    "train_batches = 0\n",
    "train_samples_effective = 0\n",
    "for images, labels in train_dataset:\n",
    "    train_batches += 1\n",
    "    train_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in train_dataset: {train_batches}\")\n",
    "print(f\"Effective number of samples in train_dataset after filtering: {train_samples_effective}\")\n",
    "\n",
    "if train_batches > 0:\n",
    "    for images, labels in train_dataset.take(1):\n",
    "        print(\"Sample batch shape from train_dataset:\", images.shape, labels.shape)\n",
    "else:\n",
    "    print(\"Warning: train_dataset is empty after filtering. Check for widespread image loading issues.\")\n",
    "\n",
    "val_batches = 0\n",
    "val_samples_effective = 0\n",
    "for images, labels in val_dataset:\n",
    "    val_batches +=1\n",
    "    val_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in val_dataset: {val_batches}\")\n",
    "print(f\"Effective number of samples in val_dataset after filtering: {val_samples_effective}\")\n",
    "\n",
    "\n",
    "test_batches = 0\n",
    "test_samples_effective = 0\n",
    "for images, labels in test_dataset:\n",
    "    test_batches += 1\n",
    "    test_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in test_dataset: {test_batches}\")\n",
    "print(f\"Effective number of samples in test_dataset after filtering: {test_samples_effective}\")\n",
    "\n",
    "\n",
    "# Check if any dataset is empty, which could cause issues during training/evaluation\n",
    "if train_samples_effective == 0 or val_samples_effective == 0:\n",
    "    print(\"CRITICAL WARNING: Training or Validation dataset is empty after processing. Model training cannot proceed effectively.\")\n",
    "    # Depending on the severity, you might want to raise an error here\n",
    "    # raise ValueError(\"Training or Validation dataset is empty.\")\n",
    "\n",
    "\n",
    "# %%\n",
    "# --- 2. Model Architecture ---\n",
    "print(\"\\nPhase 2: Building the Model\")\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet',\n",
    "                            input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "base_model.trainable = False # Start with base model frozen\n",
    "\n",
    "inputs = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "x = Dropout(0.3, name=\"top_dropout_1\")(x)\n",
    "x = Dense(128, activation='relu', name=\"dense_128\")(x)\n",
    "x = Dropout(0.3, name=\"top_dropout_2\")(x)\n",
    "outputs = Dense(1, activation='sigmoid', name=\"predictions\")(x)\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# %%\n",
    "# --- 3. Model Compilation ---\n",
    "print(\"\\nPhase 3: Compiling the Model\")\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# MODIFIED: Adjust loss based on number of classes\n",
    "if len(target_names) <= 2: # Binary classification (or single class if an error, but usually benign/malignant)\n",
    "    print(\"Binary classification\")\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    # For binary, ensure 'accuracy' is suitable. AUC, Precision, Recall are fine.\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "               tf.keras.metrics.Precision(name='precision'), \n",
    "               tf.keras.metrics.Recall(name='recall'), \n",
    "               tf.keras.metrics.F1Score(name='f1_score'),\n",
    "               tf.keras.metrics.FalseNegatives(name='false_negatives'),\n",
    "               tf.keras.metrics.FalsePositives(name='false_positives')]\n",
    "else: # Multiclass classification\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy() # Assuming y_train, etc., are integer labels\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name='auc')] # AUC might need multi_label=True or specific setup for multiclass\n",
    "    # For multiclass, typical metrics are accuracy, sparse_categorical_accuracy.\n",
    "    # Precision and Recall can be more complex (e.g., weighted, macro).\n",
    "    # For simplicity, starting with accuracy and AUC.\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.summary()\n",
    "\n",
    "print(\"Available metrics names:\", model.metrics_names)\n",
    "\n",
    "# %%\n",
    "print(\"\\nPhase 4: Training the Model (Head Only)\")\n",
    "# Ensure datasets are not empty before starting training\n",
    "if train_samples_effective == 0 or val_samples_effective == 0:\n",
    "    print(\"ERROR: Cannot start head training because train or validation dataset is empty.\")\n",
    "else:\n",
    "\n",
    "    checkpoint_filepath_head = os.path.join(OUTPUT_DIR, 'best_model_head_only.keras')\n",
    "    callbacks_head = [\n",
    "        ModelCheckpoint(filepath=checkpoint_filepath_head, save_weights_only=False, monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "        EarlyStopping(monitor='val_accuracy', patience=PATIENCE_EARLY_STOPPING, mode='max', restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=1e-7, mode='min')\n",
    "    ]\n",
    "    history_head = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks_head\n",
    "    )\n",
    "    print(\"Loading best weights from head training...\")\n",
    "    if os.path.exists(checkpoint_filepath_head):\n",
    "        model.load_weights(checkpoint_filepath_head)\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint file {checkpoint_filepath_head} not found. Using last model weights.\")\n",
    "\n",
    "\n",
    "# --- 4b. Fine-tuning Phase ---\n",
    "print(\"\\nPhase 4b: Fine-tuning (Unfreezing some base model layers)\")\n",
    "base_model.trainable = True\n",
    "\n",
    "# Unfreeze layers from a certain block onwards in EfficientNetB0\n",
    "# EfficientNetB0 has blocks named 'block2a_expand_conv', 'block3a_expand_conv', ..., 'block7a_expand_conv'\n",
    "# We can choose to unfreeze from 'block6a' or 'block5a' onwards\n",
    "# For this example, let's unfreeze from 'block5a' onwards.\n",
    "# You might need to inspect base_model.summary() to choose the right layers.\n",
    "\n",
    "# Fine-tuning strategy: Unfreeze more layers\n",
    "# Set base_model.trainable = True first\n",
    "# Then, selectively re-freeze earlier layers if desired\n",
    "# For EfficientNet, it's common to unfreeze the top blocks.\n",
    "\n",
    "fine_tune_at_layer_name = 'block6a_expand_conv' # Here we are just unfrezzing one\n",
    "set_trainable = False\n",
    "for layer in base_model.layers:\n",
    "    if layer.name == fine_tune_at_layer_name:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization): # Keep BN frozen\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False # Explicitly keep BN frozen\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "optimizer_fine_tune = Adam(learning_rate=LEARNING_RATE / 10) # Use a smaller LR\n",
    "model.compile(optimizer=optimizer_fine_tune, loss=loss, metrics=metrics) # Re-compile\n",
    "model.summary() # Show summary with new trainable params\n",
    "if train_samples_effective == 0 or val_samples_effective == 0:\n",
    "     print(\"ERROR: Cannot start fine-tuning because train or validation dataset is empty.\")\n",
    "else:\n",
    "\n",
    "    checkpoint_filepath_finetune = os.path.join(OUTPUT_DIR, 'best_model_finetuned.keras')\n",
    "    callbacks_finetune = [\n",
    "        ModelCheckpoint(filepath=checkpoint_filepath_finetune, save_weights_only=False, monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "        EarlyStopping(monitor='val_accuracy', patience=PATIENCE_EARLY_STOPPING_FT, mode='max', restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR_FT, min_lr=1e-8, mode='min')\n",
    "    ]\n",
    "\n",
    "    # Determine initial epoch for fine-tuning\n",
    "    initial_fine_tune_epoch = 0\n",
    "    if 'history_head' in locals() and hasattr(history_head, 'epoch') and history_head.epoch:\n",
    "        initial_fine_tune_epoch = history_head.epoch[-1] + 1\n",
    "    else: # If head training was skipped or history is unavailable\n",
    "        initial_fine_tune_epoch = 0 # Or EPOCHS if you want to assume head training ran for all its epochs\n",
    "        EPOCHS = 0 # Ensure we don't re-run head training if it was skipped\n",
    "\n",
    "    history_fine_tune = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS + FINE_TUNE_EPOCHS, # Total epochs\n",
    "        initial_epoch=initial_fine_tune_epoch, # Continue from where head training left off\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks_finetune\n",
    "    )\n",
    "    print(\"Loading best weights from fine-tuning...\")\n",
    "    if os.path.exists(checkpoint_filepath_finetune):\n",
    "        model.load_weights(checkpoint_filepath_finetune)\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint file {checkpoint_filepath_finetune} not found. Using last model weights from fine-tuning.\")\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# --- 5. Model Evaluation ---\n",
    "print(\"\\nPhase 5: Evaluating the Model on Test Set\")\n",
    "\n",
    "if test_samples_effective == 0:\n",
    "    print(\"ERROR: Test dataset is empty. Cannot evaluate model.\")\n",
    "    test_accuracy = 0\n",
    "    test_auc = 0\n",
    "else:\n",
    "    results = model.evaluate(test_dataset, verbose=1)\n",
    "    \n",
    "    # The F1-score is at index 5, so we need at least 6 items in results\n",
    "    print(f\"results: {results}\")\n",
    "    if len(results) >= 6:\n",
    "        final_loss = results[0]\n",
    "        final_acc = results[1]\n",
    "        final_auc = results[2]\n",
    "        final_precision = results[3]\n",
    "        final_recall = results[4]\n",
    "        final_f1_score = results[5] # <-- Extract the F1-score here\n",
    "\n",
    "        print(f\"Final Loss: {final_loss}\")\n",
    "        print(f\"Final Accuracy: {final_acc}\")\n",
    "        print(f\"Final AUC: {final_auc}\")\n",
    "        print(f\"Final Precision: {final_precision}\")\n",
    "        print(f\"Final Recall: {final_recall}\")\n",
    "        print(f\"Final F1-Score: {final_f1_score}\") # <-- Print it for confirmation\n",
    "\n",
    "        # MODIFIED: Update the filename to include loss and F1-score\n",
    "        history_plot_filename = f\"training_history_Loss{final_loss:.3f}_Acc{final_acc:.3f}_AUC{final_auc:.3f}_F1{final_f1_score:.3f}_Loss{final_loss}.png\"\n",
    "        \n",
    "    else:\n",
    "        print(\"Error: Not enough metrics returned from model.evaluate to extract F1-score.\")\n",
    "        # Fallback filename if F1-score isn't available\n",
    "        final_acc = results[1]\n",
    "        final_auc = results[2]\n",
    "        history_plot_filename = f\"training_history_Acc{final_acc:.3f}_AUC{final_auc:.3f}.png\"\n",
    "\n",
    "\n",
    "    print(\"\\nFull evaluation results:\", results)\n",
    "    print(\"Model metrics names:\", model.metrics_names)\n",
    "    y_pred_proba = model.predict(test_dataset)\n",
    "\n",
    "    # Extract true labels correctly, regardless of whether test_dataset was batched\n",
    "    y_true_test = []\n",
    "    for _, labels_batch in test_dataset.unbatch().batch(BATCH_SIZE): # Re-batch after unbatching to iterate easily\n",
    "        y_true_test.extend(labels_batch.numpy())\n",
    "    y_true_test = np.array(y_true_test)\n",
    "\n",
    "    if len(target_names) <= 2: # Binary classification\n",
    "        y_pred_classes = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "    else: # Multiclass classification\n",
    "        y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "\n",
    "    if len(y_true_test) == 0:\n",
    "        print(\"Warning: No true labels extracted from the test set. Cannot generate classification report or confusion matrix.\")\n",
    "    elif len(y_true_test) != len(y_pred_classes):\n",
    "         print(f\"Warning: Mismatch in number of true labels ({len(y_true_test)}) and predicted classes ({len(y_pred_classes)}). Skipping report/matrix.\")\n",
    "    else:\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        print(classification_report(y_true_test, y_pred_classes, target_names=target_names, labels=range(len(target_names))))\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(y_true_test, y_pred_classes, labels=range(len(target_names)))\n",
    "        print(\"\\nConfusion Matrix (Test Set):\")\n",
    "        print(cm)\n",
    "\n",
    "        plt.figure(figsize=(8,8)) # Made figure a bit bigger\n",
    "        ax = plt.gca()\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d')\n",
    "        plt.title('Confusion Matrix (Test Set)', fontsize=18)\n",
    "        \n",
    "    \n",
    "        cm_save_path = os.path.join(OUTPUT_DIR, \"confusion_matrix.png\")\n",
    "        plt.savefig(cm_save_path)\n",
    "        print(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot Training History (combined)\n",
    "# Ensure histories exist before trying to plot\n",
    "acc, val_acc, loss_hist, val_loss_hist, auc, val_auc = [], [], [], [], [], []\n",
    "f1, val_f1 = [], []\n",
    "false_positives, val_false_positives = [], []\n",
    "false_negatives, val_false_negatives = [], []\n",
    "\n",
    "epochs_range_head_len = 0\n",
    "print(f\"history_head.history: {history_head.history}\")\n",
    "if 'history_head' in locals() and hasattr(history_head, 'history'):\n",
    "    acc.extend(history_head.history.get('accuracy', []))\n",
    "    val_acc.extend(history_head.history.get('val_accuracy', []))\n",
    "    loss_hist.extend(history_head.history.get('loss', []))\n",
    "    val_loss_hist.extend(history_head.history.get('val_loss', []))\n",
    "    auc.extend(history_head.history.get('auc', []))\n",
    "    val_auc.extend(history_head.history.get('val_auc', []))\n",
    "    epochs_range_head_len = len(history_head.history.get('accuracy', []))\n",
    "    f1.extend(history_head.history.get('f1_score', []))\n",
    "    val_f1.extend(history_head.history.get('val_f1_score', []))\n",
    "    false_positives.extend(history_head.history.get('false_positives', []))\n",
    "    val_false_positives.extend(history_head.history.get('val_false_positives', []))\n",
    "    false_negatives.extend(history_head.history.get('false_negatives', []))\n",
    "    val_false_negatives.extend(history_head.history.get('val_false_negatives', []))\n",
    "\n",
    "if 'history_fine_tune' in locals() and hasattr(history_fine_tune, 'history'):\n",
    "    acc.extend(history_fine_tune.history.get('accuracy', []))\n",
    "    val_acc.extend(history_fine_tune.history.get('val_accuracy', []))\n",
    "    loss_hist.extend(history_fine_tune.history.get('loss', []))\n",
    "    val_loss_hist.extend(history_fine_tune.history.get('val_loss', []))\n",
    "    auc.extend(history_fine_tune.history.get('auc', []))\n",
    "    val_auc.extend(history_fine_tune.history.get('val_auc', []))\n",
    "    f1.extend(history_fine_tune.history.get('f1_score', []))\n",
    "    val_f1.extend(history_fine_tune.history.get('val_f1_score', []))\n",
    "    false_positives.extend(history_fine_tune.history.get('false_positives', []))\n",
    "    val_false_positives.extend(history_fine_tune.history.get('val_false_positives', []))\n",
    "    false_negatives.extend(history_fine_tune.history.get('false_negatives', []))\n",
    "    val_false_negatives.extend(history_fine_tune.history.get('val_false_negatives', []))\n",
    "\n",
    "epochs_range_total = range(len(acc))\n",
    "\n",
    "if epochs_range_total: # Only plot if there's history\n",
    "    plt.figure(figsize=(24, 16)) # Made figure wider\n",
    "    \n",
    "    # Plot 1: Accuracy\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs_range_total, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range_total, val_acc, label='Validation Accuracy')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # Plot 2: Loss\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs_range_total, loss_hist, label='Training Loss')\n",
    "    plt.plot(epochs_range_total, val_loss_hist, label='Validation Loss')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    # Plot 3: AUC\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs_range_total, auc, label='Training AUC')\n",
    "    plt.plot(epochs_range_total, val_auc, label='Validation AUC')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "\n",
    "    # Plot 4: F1-Score\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs_range_total, f1, label='Training F1')\n",
    "    plt.plot(epochs_range_total, val_f1, label='Validation F1')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation F1-Score')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('F1-Score')\n",
    "\n",
    "    # Plot 5: False Positives\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(epochs_range_total, false_positives, label='Training False Positives')\n",
    "    plt.plot(epochs_range_total, val_false_positives, label='Validation False Positives')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation False Positives')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Count')\n",
    "    plt.yscale('log')  # Log scale for better visualization\n",
    "\n",
    "    # Plot 6: False Negatives\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(epochs_range_total, false_negatives, label='Training False Negatives')\n",
    "    plt.plot(epochs_range_total, val_false_negatives, label='Validation False Negatives')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation False Negatives')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Count')\n",
    "    plt.yscale('log')  # Log scale for better visualization\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    #print(\"Available metrics names:\", model.metrics_names)\n",
    "    # Use the metrics already extracted in the evaluation section\n",
    "    final_acc = results[1]  # Accuracy\n",
    "    final_auc = results[2]  # AUC\n",
    "    history_plot_filename = f\"training_history{final_acc:.3f}_AUC{final_auc:.3f}_F1{final_f1_score:.3f}_Loss{final_loss}.png\"\n",
    "    history_save_path = os.path.join(OUTPUT_DIR, history_plot_filename)\n",
    "    plt.savefig(history_save_path)\n",
    "    print(f\"Training history plot saved to {history_save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history found to plot.\")\n",
    "\n",
    "print(\"\\n--- End of Training ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
