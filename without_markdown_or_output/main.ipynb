{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4086d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0 # Using a smaller EfficientNet variant\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration & Constants ---\n",
    "BASE_DATASET_PATH = './k_CBIS-DDSM/'\n",
    "METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'calc_case(with_jpg_img).csv')\n",
    "\n",
    "IMAGE_ROOT_DIR = BASE_DATASET_PATH\n",
    "ACTUAL_IMAGE_FILES_BASE_DIR = os.path.join(IMAGE_ROOT_DIR, 'jpg_img')\n",
    "\n",
    "# Column in CSV that conceptually should point to ROIs, even if paths are flawed\n",
    "CONCEPTUAL_ROI_COLUMN_NAME = 'jpg_ROI_img_path'\n",
    "PATHOLOGY_COLUMN_NAME = 'pathology'\n",
    "\n",
    "# Model & Training Parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224 # EfficientNetB0 default input size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50 # Initial epochs for head training\n",
    "FINE_TUNE_EPOCHS = 20 # Additional epochs for fine-tuning\n",
    "LEARNING_RATE = 1e-4\n",
    "RANDOM_STATE = 42\n",
    "# --- End of Configuration & Constants ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Phase 1: Data Loading and Preprocessing with HEURISTIC PATH FINDING ---\n",
    "\n",
    "print(\"--- Initial Path Configuration Debug ---\")\n",
    "print(f\"Current working directory (CWD): {os.getcwd()}\")\n",
    "print(f\"BASE_DATASET_PATH (relative from CWD as defined): {BASE_DATASET_PATH}\")\n",
    "print(f\"METADATA_CSV_PATH (relative from CWD as defined): {METADATA_CSV_PATH}\")\n",
    "print(f\"IMAGE_ROOT_DIR (relative from CWD as defined): {IMAGE_ROOT_DIR}\")\n",
    "print(f\"ACTUAL_IMAGE_FILES_BASE_DIR (relative from CWD as defined): {ACTUAL_IMAGE_FILES_BASE_DIR}\")\n",
    "\n",
    "# Resolve to absolute paths for clarity and checking\n",
    "abs_base_dataset_path = os.path.abspath(BASE_DATASET_PATH)\n",
    "abs_metadata_csv_path = os.path.abspath(METADATA_CSV_PATH)\n",
    "abs_image_root_dir = os.path.abspath(IMAGE_ROOT_DIR)\n",
    "abs_actual_image_files_base_dir = os.path.abspath(ACTUAL_IMAGE_FILES_BASE_DIR)\n",
    "\n",
    "print(f\"\\nResolved BASE_DATASET_PATH to absolute: {abs_base_dataset_path}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_base_dataset_path)} | Is Dir? {os.path.isdir(abs_base_dataset_path)}\")\n",
    "\n",
    "print(f\"Resolved METADATA_CSV_PATH to absolute: {abs_metadata_csv_path}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_metadata_csv_path)} | Is File? {os.path.isfile(abs_metadata_csv_path)}\")\n",
    "\n",
    "print(f\"Resolved IMAGE_ROOT_DIR to absolute: {abs_image_root_dir}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_image_root_dir)} | Is Dir? {os.path.isdir(abs_image_root_dir)}\")\n",
    "\n",
    "print(f\"Resolved ACTUAL_IMAGE_FILES_BASE_DIR (where series folders should be): {abs_actual_image_files_base_dir}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_actual_image_files_base_dir)} | Is Dir? {os.path.isdir(abs_actual_image_files_base_dir)}\")\n",
    "\n",
    "if os.path.exists(abs_actual_image_files_base_dir) and os.path.isdir(abs_actual_image_files_base_dir):\n",
    "    print(f\"\\nSample contents of ACTUAL_IMAGE_FILES_BASE_DIR ('{abs_actual_image_files_base_dir}') (first 10 items):\")\n",
    "    try:\n",
    "        sample_contents = os.listdir(abs_actual_image_files_base_dir)[:10]\n",
    "        if not sample_contents:\n",
    "            print(\"    -> Directory is empty or unreadable.\")\n",
    "        for item_idx, item in enumerate(sample_contents):\n",
    "            item_abs_path = os.path.join(abs_actual_image_files_base_dir, item)\n",
    "            item_type = \"Dir\" if os.path.isdir(item_abs_path) else \"File\" if os.path.isfile(item_abs_path) else \"Other\"\n",
    "            print(f\"    -> [{item_type}] {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Could not list directory contents: {e}\")\n",
    "else:\n",
    "    print(\"\\nCRITICAL WARNING: ACTUAL_IMAGE_FILES_BASE_DIR does not exist or is not a directory. Path searches will fail.\")\n",
    "print(\"--- End of Initial Path Configuration Debug ---\\n\")\n",
    "\n",
    "\n",
    "print(\"Proceeding with CSV loading...\")\n",
    "try:\n",
    "    source_df = pd.read_csv(METADATA_CSV_PATH)\n",
    "    print(f\"Successfully loaded {METADATA_CSV_PATH}\")\n",
    "    print(f\"Columns available: {source_df.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: CSV file not found at {METADATA_CSV_PATH}.\")\n",
    "    print(\"Verify METADATA_CSV_PATH definition and file existence based on the debug info above.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")\n",
    "    raise\n",
    "\n",
    "# Clean and filter initial dataframe\n",
    "if CONCEPTUAL_ROI_COLUMN_NAME not in source_df.columns or PATHOLOGY_COLUMN_NAME not in source_df.columns:\n",
    "    print(f\"ERROR: Required columns for metadata ('{CONCEPTUAL_ROI_COLUMN_NAME}' or '{PATHOLOGY_COLUMN_NAME}') not in CSV.\")\n",
    "    print(f\"Available columns are: {source_df.columns.tolist()}\")\n",
    "    raise KeyError(\"Missing essential columns in CSV.\")\n",
    "\n",
    "source_df.dropna(subset=[CONCEPTUAL_ROI_COLUMN_NAME, PATHOLOGY_COLUMN_NAME], inplace=True)\n",
    "source_df = source_df[source_df[PATHOLOGY_COLUMN_NAME].isin(['MALIGNANT', 'BENIGN'])]\n",
    "print(f\"Rows after initial cleaning (dropna on conceptual ROI/pathology, pathology filter): {len(source_df)}\")\n",
    "\n",
    "if source_df.empty:\n",
    "    raise ValueError(\"DataFrame is empty after initial cleaning. Cannot proceed.\")\n",
    "\n",
    "def heuristic_find_image_path(row, actual_images_root_dir_abs):\n",
    "    try:\n",
    "        patient_id = row['patient_id']\n",
    "        breast_side = row['left or right breast']\n",
    "        image_view = row['image view']\n",
    "        abnormality_id = str(row['abnormality id']) # Ensure it's a string for concatenation\n",
    "\n",
    "        csv_conceptual_roi_path = str(row.get(CONCEPTUAL_ROI_COLUMN_NAME, \"\")).strip()\n",
    "        \n",
    "        case_type_folder_prefix = \"\"\n",
    "        if csv_conceptual_roi_path.startswith(\"jpg_img/\"):\n",
    "            path_part = csv_conceptual_roi_path.split('/')[1] # e.g., \"Calc_Training_P_00005_...\"\n",
    "            # Extract the part before patient_id\n",
    "            if path_part.startswith(\"Calc_Training_\"): case_type_folder_prefix = \"Calc_Training\"\n",
    "            elif path_part.startswith(\"Calc_Test_\"): case_type_folder_prefix = \"Calc_Test\"\n",
    "            elif path_part.startswith(\"Mass_Training_\"): case_type_folder_prefix = \"Mass_Training\"\n",
    "            elif path_part.startswith(\"Mass_Test_\"): case_type_folder_prefix = \"Mass_Test\"\n",
    "        \n",
    "        if not case_type_folder_prefix:\n",
    "            # print(f\"DEBUG (heuristic): Could not determine case_type_folder_prefix for {patient_id} from '{csv_conceptual_roi_path}'\")\n",
    "            return None\n",
    "\n",
    "        # Form search pattern for directories: e.g., /path/to/jpg_img/Calc_Training_P_00005_RIGHT_CC_1-*\n",
    "        dir_search_prefix = f\"{case_type_folder_prefix}_{patient_id}_{breast_side}_{image_view}_{abnormality_id}\"\n",
    "        full_dir_search_pattern = os.path.join(actual_images_root_dir_abs, f\"{dir_search_prefix}-*\")\n",
    "        \n",
    "        potential_series_dirs = glob.glob(full_dir_search_pattern)\n",
    "        \n",
    "        if not potential_series_dirs:\n",
    "            # print(f\"DEBUG (heuristic): No series directory found for {patient_id} with pattern '{full_dir_search_pattern}'\")\n",
    "            return None\n",
    "\n",
    "        roi_filename_patterns = [\n",
    "            \"ROI-mask-images-img_0-*.jpg\", \"ROI-mask-images-img_1-*.jpg\", \"ROI-mask-images-img_*-*.jpg\"\n",
    "        ]\n",
    "        \n",
    "        for series_dir_on_disk in sorted(potential_series_dirs):\n",
    "            if os.path.isdir(series_dir_on_disk):\n",
    "                for pattern in roi_filename_patterns:\n",
    "                    image_search_glob = os.path.join(series_dir_on_disk, pattern)\n",
    "                    found_roi_files = glob.glob(image_search_glob)\n",
    "                    if found_roi_files:\n",
    "                        found_roi_files.sort()\n",
    "                        return found_roi_files[0] # Return the first valid ROI found\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # print(f\"DEBUG (heuristic): Error for row {row.get('patient_id', 'Unknown')}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Attempting HEURISTIC search for valid ROI paths for each CSV entry...\")\n",
    "source_df['full_image_path'] = source_df.apply(\n",
    "    lambda r: heuristic_find_image_path(r, abs_actual_image_files_base_dir), axis=1\n",
    ")\n",
    "\n",
    "metadata_df = source_df.dropna(subset=['full_image_path']).copy()\n",
    "found_image_count = len(metadata_df)\n",
    "print(f\"Found {found_image_count} actual image files (ROIs if available) after HEURISTIC search.\")\n",
    "\n",
    "if found_image_count == 0:\n",
    "    print(\"CRITICAL ERROR: Still no valid image files found even after heuristic search.\")\n",
    "    raise FileNotFoundError(\"No usable image files found even with heuristic search.\")\n",
    "\n",
    "metadata_df.rename(columns={'full_image_path': 'full_roi_path'}, inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "metadata_df.loc[:, 'pathology_encoded'] = label_encoder.fit_transform(metadata_df[PATHOLOGY_COLUMN_NAME])\n",
    "target_names = list(label_encoder.classes_)\n",
    "\n",
    "X = metadata_df['full_roi_path']\n",
    "y = metadata_df['pathology_encoded']\n",
    "print(f\"Number of samples going into train_test_split: {len(X)}\")\n",
    "\n",
    "if len(X) == 0:\n",
    "     raise ValueError(\"Dataset is empty, cannot split.\")\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, random_state=RANDOM_STATE, stratify=y_train_val\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "def load_and_preprocess_image(image_path_tensor, label_tensor):\n",
    "    # This function is wrapped by tf.py_function, so inputs are tensors.\n",
    "    # Convert image_path_tensor to a Python string for cv2.imread\n",
    "    image_path_str = image_path_tensor.numpy().decode('utf-8')\n",
    "    \n",
    "    try:\n",
    "        img = cv2.imread(image_path_str)\n",
    "        if img is None: # Check if image loading failed\n",
    "            # print(f\"Warning: Could not read image {image_path_str}. Returning dummy data.\")\n",
    "            # Return dummy data that matches expected types and shapes\n",
    "            dummy_img_array = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.float32)\n",
    "            # Preprocess even the dummy image if your model expects preprocessed input\n",
    "            dummy_img_processed = tf.keras.applications.efficientnet.preprocess_input(dummy_img_array.copy())\n",
    "            error_label = np.int32(-1) # Special label to indicate a problem\n",
    "            return dummy_img_processed, error_label\n",
    "        \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCV loads as BGR, convert to RGB\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        # Ensure img is float32 before preprocessing if preprocess_input expects that\n",
    "        img_float32 = img.astype(np.float32)\n",
    "        img_processed = tf.keras.applications.efficientnet.preprocess_input(img_float32)\n",
    "        \n",
    "        # Ensure label is also returned as the correct type (np.int32 for the tf.py_function Tout)\n",
    "        return img_processed, label_tensor.numpy().astype(np.int32) # Ensure label is also concrete numpy value for return\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing image {image_path_str}: {e}. Returning dummy data.\")\n",
    "        # Return dummy data in case of any other exception during processing\n",
    "        dummy_img_array = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.float32)\n",
    "        dummy_img_processed = tf.keras.applications.efficientnet.preprocess_input(dummy_img_array.copy())\n",
    "        error_label = np.int32(-1) # Special label\n",
    "        return dummy_img_processed, error_label\n",
    "\n",
    "\n",
    "# Data Augmentation (tf.data has better performance)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomBrightness(factor=0.1),\n",
    "    tf.keras.layers.RandomContrast(factor=0.1)\n",
    "])\n",
    "\n",
    "def create_tf_dataset(image_paths, labels, batch_size, augment=False):\n",
    "    image_paths_list = list(image_paths) # Ensure it's a list of Python strings\n",
    "    labels_list = list(labels)       # Ensure it's a list of Python ints/floats\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths_list, labels_list))\n",
    "    \n",
    "    # Map image loading and preprocessing\n",
    "    # Tensorflow expects the wrapped function to return types matching `Tout`\n",
    "    dataset = dataset.map(lambda x, y: tf.py_function(\n",
    "        load_and_preprocess_image, \n",
    "        [x, y], # Inputs to the Python function\n",
    "        [tf.float32, tf.int32]), # Output types\n",
    "        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Filter out entries marked with the error_label (-1)\n",
    "    dataset = dataset.filter(lambda img_tensor, label_tensor: label_tensor != -1)\n",
    "    \n",
    "    # All images passing the filter should now have the correct shape.\n",
    "    def set_shape(img, label):\n",
    "        img.set_shape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "        label.set_shape(()) # Label is a scalar\n",
    "        return img, label\n",
    "    dataset = dataset.map(set_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if augment:\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Recreate datasets with the modified functions\n",
    "print(\"Recreating TensorFlow datasets with updated image loading logic...\")\n",
    "train_dataset = create_tf_dataset(X_train, y_train, BATCH_SIZE, augment=True)\n",
    "val_dataset = create_tf_dataset(X_val, y_val, BATCH_SIZE, augment=False)\n",
    "test_dataset = create_tf_dataset(X_test, y_test, BATCH_SIZE, augment=False)\n",
    "\n",
    "# Verify a batch shape and count items (good here since the dataset is not too big! (diferent from the other CBIS-DDSM works))\n",
    "print(\"Verifying dataset integrity (this might take a moment)...\")\n",
    "train_count = 0\n",
    "for _ in train_dataset:\n",
    "    train_count += 1\n",
    "print(f\"Number of batches in train_dataset: {train_count}\")\n",
    "if train_count > 0:\n",
    "    for images, labels in train_dataset.take(1):\n",
    "        print(\"Sample batch shape from train_dataset:\", images.shape, labels.shape)\n",
    "else:\n",
    "    print(\"Warning: train_dataset is empty after filtering. Check for widespread image loading issues.\")\n",
    "\n",
    "val_count = 0\n",
    "for _ in val_dataset:\n",
    "    val_count +=1\n",
    "print(f\"Number of batches in val_dataset: {val_count}\")\n",
    "\n",
    "test_count = 0\n",
    "for _ in test_dataset:\n",
    "    test_count += 1\n",
    "print(f\"Number of batches in test_dataset: {test_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c15a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Model Architecture ---\n",
    "print(\"\\nPhase 2: Building the Model\")\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet',\n",
    "                            input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "base_model.trainable = False # Start with base model frozen\n",
    "\n",
    "inputs = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "x = Dropout(0.3, name=\"top_dropout_1\")(x)\n",
    "x = Dense(128, activation='relu', name=\"dense_128\")(x)\n",
    "x = Dropout(0.3, name=\"top_dropout_2\")(x)\n",
    "outputs = Dense(1, activation='sigmoid', name=\"predictions\")(x)\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Model Compilation ---\n",
    "print(\"\\nPhase 3: Compiling the Model\")\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = ['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "           tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ce229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Training the Model (Head Only) ---\n",
    "print(\"\\nPhase 4: Training the Model (Head Only)\")\n",
    "checkpoint_filepath_head = './best_model_head_only.keras'\n",
    "callbacks_head = [\n",
    "    ModelCheckpoint(filepath=checkpoint_filepath_head, save_weights_only=False, monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, mode='min')\n",
    "]\n",
    "history_head = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_head\n",
    ")\n",
    "\n",
    "print(\"Loading best weights from head training...\")\n",
    "model.load_weights(checkpoint_filepath_head) # Load best weights saved\n",
    "\n",
    "# --- 4b. Fine-tuning Phase ---\n",
    "print(\"\\nPhase 4b: Fine-tuning (Unfreezing some base model layers)\")\n",
    "base_model.trainable = True\n",
    "# for layer in base_model.layers[-30:]: # Example: unfreeze last 30 layers\n",
    "#    if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "#        layer.trainable = True\n",
    "# Keep Batch Normalization layers frozen\n",
    "for layer in base_model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "optimizer_fine_tune = Adam(learning_rate=LEARNING_RATE / 10) # Use a smaller LR\n",
    "model.compile(optimizer=optimizer_fine_tune, loss=loss, metrics=metrics) # Re-compile\n",
    "model.summary()\n",
    "\n",
    "checkpoint_filepath_finetune = './best_model_finetuned.keras'\n",
    "callbacks_finetune = [\n",
    "    ModelCheckpoint(filepath=checkpoint_filepath_finetune, save_weights_only=False, monitor='val_accuracy', mode='max', save_best_only=True),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, mode='max', restore_best_weights=True), # Can adjust patience\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-8, mode='min')\n",
    "]\n",
    "\n",
    "total_epochs_for_finetuning = EPOCHS + FINE_TUNE_EPOCHS # If you want to run for FINE_TUNE_EPOCHS more\n",
    "\n",
    "history_fine_tune = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=total_epochs_for_finetuning,\n",
    "    initial_epoch=history_head.epoch[-1] + 1, # Continue from where head training left off\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks_finetune\n",
    ")\n",
    "\n",
    "print(\"Loading best weights from fine-tuning...\")\n",
    "model.load_weights(checkpoint_filepath_finetune) # Load best weights saved from fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Model Evaluation ---\n",
    "print(\"\\nPhase 5: Evaluating the Model on Test Set\")\n",
    "results = model.evaluate(test_dataset, verbose=1)\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(f\"{name}: {value:.4f}\")\n",
    "\n",
    "y_pred_proba = model.predict(test_dataset)\n",
    "y_pred_classes = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "y_true_test = []\n",
    "for _, labels_batch in test_dataset: # Extract true labels from the tf.data.Dataset\n",
    "    y_true_test.extend(labels_batch.numpy())\n",
    "y_true_test = np.array(y_true_test)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_true_test, y_pred_classes, target_names=target_names))\n",
    "\n",
    "cm = confusion_matrix(y_true_test, y_pred_classes)\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "print(cm)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = plt.gca()\n",
    "ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix (Test Set)', fontsize=18)\n",
    "ax.set_xticks(range(len(target_names)))\n",
    "ax.set_yticks(range(len(target_names)))\n",
    "ax.set_xticklabels(target_names)\n",
    "ax.set_yticklabels(target_names)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot Training History (combined)\n",
    "acc = history_head.history['accuracy'] + history_fine_tune.history.get('accuracy', [])\n",
    "val_acc = history_head.history['val_accuracy'] + history_fine_tune.history.get('val_accuracy', [])\n",
    "loss_hist = history_head.history['loss'] + history_fine_tune.history.get('loss', [])\n",
    "val_loss_hist = history_head.history['val_loss'] + history_fine_tune.history.get('val_loss', [])\n",
    "auc = history_head.history['auc'] + history_fine_tune.history.get('auc', [])\n",
    "val_auc = history_head.history['val_auc'] + history_fine_tune.history.get('val_auc', [])\n",
    "\n",
    "epochs_range_head = range(len(history_head.history['accuracy']))\n",
    "epochs_range_total = range(len(acc))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(epochs_range_total, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range_total, val_acc, label='Validation Accuracy')\n",
    "plt.axvline(x=epochs_range_head[-1], color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(epochs_range_total, loss_hist, label='Training Loss')\n",
    "plt.plot(epochs_range_total, val_loss_hist, label='Validation Loss')\n",
    "plt.axvline(x=epochs_range_head[-1], color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(epochs_range_total, auc, label='Training AUC')\n",
    "plt.plot(epochs_range_total, val_auc, label='Validation AUC')\n",
    "plt.axvline(x=epochs_range_head[-1], color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation AUC')\n",
    "\n",
    "# Add precision and recall plots if desired\n",
    "precision = history_head.history.get('precision', []) + history_fine_tune.history.get('precision', [])\n",
    "val_precision = history_head.history.get('val_precision', []) + history_fine_tune.history.get('val_precision', [])\n",
    "recall = history_head.history.get('recall', []) + history_fine_tune.history.get('recall', [])\n",
    "val_recall = history_head.history.get('val_recall', []) + history_fine_tune.history.get('val_recall', [])\n",
    "\n",
    "if precision: # Check if precision was recorded\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.plot(epochs_range_total, precision, label='Training Precision')\n",
    "    plt.plot(epochs_range_total, val_precision, label='Validation Precision')\n",
    "    plt.axvline(x=epochs_range_head[-1], color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Precision')\n",
    "\n",
    "if recall: # Check if recall was recorded\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.plot(epochs_range_total, recall, label='Training Recall')\n",
    "    plt.plot(epochs_range_total, val_recall, label='Validation Recall')\n",
    "    plt.axvline(x=epochs_range_head[-1], color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Recall')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- End of Script ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
