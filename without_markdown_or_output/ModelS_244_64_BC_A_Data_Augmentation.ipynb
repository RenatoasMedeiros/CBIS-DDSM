{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input, Conv2D, MaxPooling2D, BatchNormalization, Rescaling\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Constantes ---\n",
    "MODEL_NAME = 'ModelS' # Renamed for clarity\n",
    "\n",
    "BASE_DATASET_PATH = './k_CBIS-DDSM/'\n",
    "CALC_METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'calc_case(with_jpg_img).csv')\n",
    "MASS_METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'mass_case(with_jpg_img).csv')\n",
    "\n",
    "IMAGE_ROOT_DIR = BASE_DATASET_PATH\n",
    "ACTUAL_IMAGE_FILES_BASE_DIR = os.path.join(IMAGE_ROOT_DIR, 'jpg_img')\n",
    "\n",
    "CONCEPTUAL_ROI_COLUMN_NAME = 'jpg_ROI_img_path'\n",
    "PATHOLOGY_COLUMN_NAME = 'pathology'\n",
    "CASE_TYPE_COLUMN_NAME = 'case_type'\n",
    "\n",
    "# Parametros de treino do modelo\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100 \n",
    "LEARNING_RATE = 1e-4\n",
    "RANDOM_STATE = 42\n",
    "PATIENCE_EARLY_STOPPING = 15\n",
    "PATIENCE_REDUCE_LR = 7\n",
    "\n",
    "OUTPUT_DIR = os.path.join('./', f\"run_{MODEL_NAME}_{IMG_WIDTH}_{BATCH_SIZE}_BC_A_DataAugmentation_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"All output will be saved to: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "# --- FIM Constantes ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Initial Path Configuration Debug ---\")\n",
    "print(f\"Current working directory (CWD): {os.getcwd()}\")\n",
    "print(f\"BASE_DATASET_PATH (relative from CWD as defined): {BASE_DATASET_PATH}\")\n",
    "print(f\"CALC_METADATA_CSV_PATH (relative from CWD as defined): {CALC_METADATA_CSV_PATH}\") \n",
    "print(f\"MASS_METADATA_CSV_PATH (relative from CWD as defined): {MASS_METADATA_CSV_PATH}\")   # ADDED\n",
    "print(f\"IMAGE_ROOT_DIR (relative from CWD as defined): {IMAGE_ROOT_DIR}\")\n",
    "print(f\"ACTUAL_IMAGE_FILES_BASE_DIR (relative from CWD as defined): {ACTUAL_IMAGE_FILES_BASE_DIR}\")\n",
    "\n",
    "abs_base_dataset_path = os.path.abspath(BASE_DATASET_PATH)\n",
    "abs_calc_metadata_csv_path = os.path.abspath(CALC_METADATA_CSV_PATH) \n",
    "abs_mass_metadata_csv_path = os.path.abspath(MASS_METADATA_CSV_PATH)   # ADDED\n",
    "abs_image_root_dir = os.path.abspath(IMAGE_ROOT_DIR)\n",
    "abs_actual_image_files_base_dir = os.path.abspath(ACTUAL_IMAGE_FILES_BASE_DIR)\n",
    "\n",
    "print(f\"\\nResolved BASE_DATASET_PATH to absolute: {abs_base_dataset_path}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_base_dataset_path)} | Is Dir? {os.path.isdir(abs_base_dataset_path)}\")\n",
    "\n",
    "print(f\"Resolved CALC_METADATA_CSV_PATH to absolute: {abs_calc_metadata_csv_path}\") \n",
    "print(f\"  -> Exists? {os.path.exists(abs_calc_metadata_csv_path)} | Is File? {os.path.isfile(abs_calc_metadata_csv_path)}\")\n",
    "\n",
    "print(f\"Resolved MASS_METADATA_CSV_PATH to absolute: {abs_mass_metadata_csv_path}\")   # ADDED\n",
    "print(f\"  -> Exists? {os.path.exists(abs_mass_metadata_csv_path)} | Is File? {os.path.isfile(abs_mass_metadata_csv_path)}\")\n",
    "\n",
    "print(f\"Resolved IMAGE_ROOT_DIR to absolute: {abs_image_root_dir}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_image_root_dir)} | Is Dir? {os.path.isdir(abs_image_root_dir)}\")\n",
    "\n",
    "print(f\"Resolved ACTUAL_IMAGE_FILES_BASE_DIR (where series folders should be): {abs_actual_image_files_base_dir}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_actual_image_files_base_dir)} | Is Dir? {os.path.isdir(abs_actual_image_files_base_dir)}\")\n",
    "\n",
    "if os.path.exists(abs_actual_image_files_base_dir) and os.path.isdir(abs_actual_image_files_base_dir):\n",
    "    print(f\"\\nSample contents of ACTUAL_IMAGE_FILES_BASE_DIR ('{abs_actual_image_files_base_dir}') (first 10 items):\")\n",
    "    try:\n",
    "        sample_contents = os.listdir(abs_actual_image_files_base_dir)[:10]\n",
    "        if not sample_contents:\n",
    "            print(\"    -> Directory is empty or unreadable.\")\n",
    "        for item_idx, item in enumerate(sample_contents):\n",
    "            item_abs_path = os.path.join(abs_actual_image_files_base_dir, item)\n",
    "            item_type = \"Dir\" if os.path.isdir(item_abs_path) else \"File\" if os.path.isfile(item_abs_path) else \"Other\"\n",
    "            print(f\"    -> [{item_type}] {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Could not list directory contents: {e}\")\n",
    "else:\n",
    "    print(\"\\nCRITICAL WARNING: ACTUAL_IMAGE_FILES_BASE_DIR does not exist or is not a directory. Path searches will fail.\")\n",
    "print(\"--- End of Initial Path Configuration Debug ---\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Proceeding with CSV loading...\")\n",
    "loaded_dfs = []\n",
    "\n",
    "# Load Calc cases\n",
    "if os.path.exists(abs_calc_metadata_csv_path):\n",
    "    try:\n",
    "        calc_df = pd.read_csv(abs_calc_metadata_csv_path)\n",
    "        calc_df[CASE_TYPE_COLUMN_NAME] = 'calc' # Add case type identifier\n",
    "        loaded_dfs.append(calc_df)\n",
    "        print(f\"Successfully loaded and tagged {len(calc_df)} rows from {CALC_METADATA_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the CALC CSV ({CALC_METADATA_CSV_PATH}): {e}\")\n",
    "else:\n",
    "    print(f\"WARNING: CALC CSV file not found at {abs_calc_metadata_csv_path}. Skipping.\")\n",
    "\n",
    "# Load Mass cases\n",
    "if os.path.exists(abs_mass_metadata_csv_path):\n",
    "    try:\n",
    "        mass_df = pd.read_csv(abs_mass_metadata_csv_path)\n",
    "        mass_df[CASE_TYPE_COLUMN_NAME] = 'mass' # Add case type identifier\n",
    "        loaded_dfs.append(mass_df)\n",
    "        print(f\"Successfully loaded and tagged {len(mass_df)} rows from {MASS_METADATA_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the MASS CSV ({MASS_METADATA_CSV_PATH}): {e}\")\n",
    "else:\n",
    "    print(f\"WARNING: MASS CSV file not found at {abs_mass_metadata_csv_path}. Skipping.\")\n",
    "\n",
    "if not loaded_dfs:\n",
    "    print(\"ERROR: No CSV files were loaded. Cannot proceed.\")\n",
    "    raise FileNotFoundError(\"Neither Calc nor Mass CSV files could be loaded. Check paths and file existence.\")\n",
    "\n",
    "source_df = pd.concat(loaded_dfs, ignore_index=True)\n",
    "print(f\"Combined DataFrame created with {len(source_df)} total rows from {len(loaded_dfs)} CSV file(s).\")\n",
    "print(f\"Columns available in combined DataFrame: {source_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "\n",
    "if CONCEPTUAL_ROI_COLUMN_NAME not in source_df.columns or PATHOLOGY_COLUMN_NAME not in source_df.columns:\n",
    "    print(f\"ERROR: Required columns for metadata ('{CONCEPTUAL_ROI_COLUMN_NAME}' or '{PATHOLOGY_COLUMN_NAME}') not in combined CSV.\")\n",
    "    print(f\"Available columns are: {source_df.columns.tolist()}\")\n",
    "    raise KeyError(\"Missing essential columns in combined CSV.\")\n",
    "\n",
    "source_df.dropna(subset=[CONCEPTUAL_ROI_COLUMN_NAME, PATHOLOGY_COLUMN_NAME], inplace=True)\n",
    "source_df = source_df[source_df[PATHOLOGY_COLUMN_NAME].isin(['MALIGNANT', 'BENIGN'])]\n",
    "print(f\"Rows after initial cleaning (dropna on conceptual ROI/pathology, pathology filter): {len(source_df)}\")\n",
    "\n",
    "if source_df.empty:\n",
    "    raise ValueError(\"Combined DataFrame is empty after initial cleaning. Cannot proceed.\")\n",
    "\n",
    "def heuristic_find_image_path(row, actual_images_root_dir_abs):\n",
    "    try:\n",
    "        patient_id = row['patient_id']\n",
    "        breast_side = row['left or right breast']\n",
    "        image_view = row['image view']\n",
    "        abnormality_id = str(row['abnormality id']) \n",
    "\n",
    "        csv_conceptual_roi_path = str(row.get(CONCEPTUAL_ROI_COLUMN_NAME, \"\")).strip()\n",
    "\n",
    "        case_type_folder_prefix = \"\"\n",
    "        if csv_conceptual_roi_path.startswith(\"jpg_img/\"):\n",
    "            path_part = csv_conceptual_roi_path.split('/')[1] \n",
    "            \n",
    "            \n",
    "            if path_part.startswith(\"Calc_Training_\"): case_type_folder_prefix = \"Calc_Training\"\n",
    "            elif path_part.startswith(\"Calc_Test_\"): case_type_folder_prefix = \"Calc_Test\"\n",
    "            elif path_part.startswith(\"Mass_Training_\"): case_type_folder_prefix = \"Mass_Training\"\n",
    "            elif path_part.startswith(\"Mass_Test_\"): case_type_folder_prefix = \"Mass_Test\"\n",
    "\n",
    "        if not case_type_folder_prefix:\n",
    "            # print(f\"DEBUG (heuristic): Could not determine case_type_folder_prefix for {patient_id} from '{csv_conceptual_roi_path}'\")\n",
    "            return None\n",
    "\n",
    "        dir_search_prefix = f\"{case_type_folder_prefix}_{patient_id}_{breast_side}_{image_view}_{abnormality_id}\"\n",
    "        full_dir_search_pattern = os.path.join(actual_images_root_dir_abs, f\"{dir_search_prefix}-*\")\n",
    "\n",
    "        potential_series_dirs = glob.glob(full_dir_search_pattern)\n",
    "\n",
    "        if not potential_series_dirs:\n",
    "            # print(f\"DEBUG (heuristic): No series directory found for {patient_id} with pattern '{full_dir_search_pattern}'\")\n",
    "            return None\n",
    "\n",
    "        roi_filename_patterns = [\n",
    "            \"ROI-mask-images-img_0-*.jpg\", \"ROI-mask-images-img_1-*.jpg\", \"ROI-mask-images-img_*-*.jpg\"\n",
    "        ]\n",
    "\n",
    "        for series_dir_on_disk in sorted(potential_series_dirs): \n",
    "            if os.path.isdir(series_dir_on_disk):\n",
    "                for pattern in roi_filename_patterns:\n",
    "                    image_search_glob = os.path.join(series_dir_on_disk, pattern)\n",
    "                    found_roi_files = glob.glob(image_search_glob)\n",
    "                    if found_roi_files:\n",
    "                        found_roi_files.sort() \n",
    "                        return found_roi_files[0] \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # print(f\"DEBUG (heuristic): Error for row {row.get('patient_id', 'Unknown')} ({row.get(CASE_TYPE_COLUMN_NAME, 'N/A')} case): {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Attempting HEURISTIC search for valid ROI paths for each CSV entry...\")\n",
    "source_df['full_image_path'] = source_df.apply(\n",
    "    lambda r: heuristic_find_image_path(r, abs_actual_image_files_base_dir), axis=1\n",
    ")\n",
    "\n",
    "metadata_df = source_df.dropna(subset=['full_image_path']).copy()\n",
    "found_image_count = len(metadata_df)\n",
    "print(f\"Found {found_image_count} actual image files (ROIs if available) after HEURISTIC search from combined data.\")\n",
    "print(f\"Breakdown by case type (if available in metadata_df): \\n{metadata_df[CASE_TYPE_COLUMN_NAME].value_counts()}\")\n",
    "\n",
    "\n",
    "if found_image_count == 0:\n",
    "    print(\"CRITICAL ERROR: Still no valid image files found even after heuristic search from combined data.\")\n",
    "    raise FileNotFoundError(\"No usable image files found even with heuristic search from combined data.\")\n",
    "\n",
    "metadata_df.rename(columns={'full_image_path': 'full_roi_path'}, inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "metadata_df.loc[:, 'pathology_encoded'] = label_encoder.fit_transform(metadata_df[PATHOLOGY_COLUMN_NAME])\n",
    "target_names = list(label_encoder.classes_)\n",
    "\n",
    "X = metadata_df['full_roi_path']\n",
    "y = metadata_df['pathology_encoded']\n",
    "print(f\"Number of samples going into train_test_split: {len(X)}\")\n",
    "\n",
    "if len(X) == 0:\n",
    "    raise ValueError(\"Dataset is empty, cannot split.\")\n",
    "\n",
    "# Stratify by y to ensure balanced splits, especially important if classes are imbalanced\n",
    "# or if combining datasets leads to different proportions.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, random_state=RANDOM_STATE, stratify=y_train_val # Stratify this split too\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Print class distribution in each set to verify stratification\n",
    "print(f\"Train labels distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Validation labels distribution: {np.bincount(y_val)}\")\n",
    "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "\n",
    "def load_image(image_path_tensor, label_tensor):\n",
    "    image_path_str = image_path_tensor.numpy().decode('utf-8')\n",
    "    try:\n",
    "        img = cv2.imread(image_path_str)\n",
    "        if img is None:\n",
    "            return np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8), np.int32(-1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        return img, label_tensor.numpy().astype(np.int32)\n",
    "    except Exception as e:\n",
    "        return np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8), np.int32(-1)\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "def create_tf_dataset(image_paths, labels, batch_size, augment=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(image_paths), list(labels)))\n",
    "    dataset = dataset.map(lambda x, y: tf.py_function(load_image, [x, y], [tf.uint8, tf.int32]),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.filter(lambda img, label: label != -1)\n",
    "\n",
    "    def set_shape(img, label):\n",
    "        img.set_shape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "        label.set_shape(())\n",
    "        return img, label\n",
    "    dataset = dataset.map(set_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    if augment:\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def visualize_augmentations(sample_image_paths, augmentation_layer, output_dir):\n",
    "    print(\"Generating and saving data augmentation visualization...\")\n",
    "    num_examples = len(sample_image_paths)\n",
    "    num_augmentations = 4 \n",
    "    \n",
    "    plt.figure(figsize=(5 * num_augmentations, 5 * num_examples))\n",
    "    \n",
    "    for i, image_path in enumerate(sample_image_paths):\n",
    "        # Load the original image using a simplified version of the preprocessing\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        \n",
    "        # Display the original image\n",
    "        ax = plt.subplot(num_examples, num_augmentations + 1, i * (num_augmentations + 1) + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Original {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        # Add batch dimension and apply augmentations\n",
    "        img_tensor = tf.expand_dims(tf.convert_to_tensor(img), 0)\n",
    "        for j in range(num_augmentations):\n",
    "            augmented_image = augmentation_layer(img_tensor, training=True)\n",
    "            ax = plt.subplot(num_examples, num_augmentations + 1, i * (num_augmentations + 1) + j + 2)\n",
    "            plt.imshow(tf.squeeze(augmented_image, axis=0).numpy().astype(\"uint8\"))\n",
    "            plt.title(f\"Augmented {j+1}\")\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    # Save the figure to the specified output directory\n",
    "    save_path = os.path.join(output_dir, \"data_augmentation_examples.png\")\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Augmentation visualization saved to {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# --- Call the new visualization function ---\n",
    "num_viz_samples = 3\n",
    "if len(X_train) >= num_viz_samples:\n",
    "    visualize_augmentations(\n",
    "        sample_image_paths=X_train.iloc[:num_viz_samples],\n",
    "        augmentation_layer=data_augmentation,\n",
    "        output_dir=OUTPUT_DIR\n",
    "    )\n",
    "\n",
    "print(\"Recreating TensorFlow datasets with updated image loading logic...\")\n",
    "train_dataset = create_tf_dataset(X_train, y_train, BATCH_SIZE, augment=True)\n",
    "val_dataset = create_tf_dataset(X_val, y_val, BATCH_SIZE, augment=False)\n",
    "test_dataset = create_tf_dataset(X_test, y_test, BATCH_SIZE, augment=False)\n",
    "\n",
    "print(\"Verifying dataset integrity (this might take a moment)...\")\n",
    "train_batches = 0\n",
    "train_samples_effective = 0\n",
    "for images, labels in train_dataset:\n",
    "    train_batches += 1\n",
    "    train_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in train_dataset: {train_batches}\")\n",
    "print(f\"Effective number of samples in train_dataset after filtering: {train_samples_effective}\")\n",
    "\n",
    "if train_batches > 0:\n",
    "    for images, labels in train_dataset.take(1):\n",
    "        print(\"Sample batch shape from train_dataset:\", images.shape, labels.shape)\n",
    "else:\n",
    "    print(\"Warning: train_dataset is empty after filtering. Check for widespread image loading issues.\")\n",
    "\n",
    "val_batches = 0\n",
    "val_samples_effective = 0\n",
    "for images, labels in val_dataset:\n",
    "    val_batches +=1\n",
    "    val_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in val_dataset: {val_batches}\")\n",
    "print(f\"Effective number of samples in val_dataset after filtering: {val_samples_effective}\")\n",
    "\n",
    "\n",
    "test_batches = 0\n",
    "test_samples_effective = 0\n",
    "for images, labels in test_dataset:\n",
    "    test_batches += 1\n",
    "    test_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in test_dataset: {test_batches}\")\n",
    "print(f\"Effective number of samples in test_dataset after filtering: {test_samples_effective}\")\n",
    "\n",
    "\n",
    "# Check if any dataset is empty, which could cause issues during training/evaluation\n",
    "if train_samples_effective == 0 or val_samples_effective == 0:\n",
    "    print(\"CRITICAL WARNING: Training or Validation dataset is empty after processing. Model training cannot proceed effectively.\")\n",
    "    # Depending on the severity, you might want to raise an error here\n",
    "    # raise ValueError(\"Training or Validation dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Arquitetura do Modelo S ---\n",
    "print(\"\\nPhase 2: Building Model S\")\n",
    "\n",
    "inputs = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "\n",
    "# Normalização entre 0 e 1.\n",
    "x = Rescaling(1./255)(inputs)\n",
    "\n",
    "# --- Base ---\n",
    "x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# --- End of the Base ---\n",
    "\n",
    "# --- Start of the Classifier Head ---\n",
    "x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "x = Dropout(0.5, name=\"top_dropout_1\")(x)\n",
    "x = Dense(128, activation='relu', name=\"dense_128\")(x)\n",
    "x = Dropout(0.5, name=\"top_dropout_2\")(x)\n",
    "outputs = Dense(1, activation='sigmoid', name=\"predictions\")(x)\n",
    "# --- End of the Classifier Head ---\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Compilação do Modelo ---\n",
    "print(\"\\nPhase 3: Compiling the Model\")\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "metrics = ['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Treino do Modelo ---\n",
    "print(\"\\nPhase 4: Training the Custom Model\")\n",
    "\n",
    "checkpoint_filepath = os.path.join(OUTPUT_DIR, 'best_model_from_scratch.keras')\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_auc', mode='max', save_best_only=True),\n",
    "    EarlyStopping(monitor='val_auc', patience=PATIENCE_EARLY_STOPPING, mode='max', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=PATIENCE_REDUCE_LR, min_lr=0.01, mode='min')\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"Loading best weights from training...\")\n",
    "if os.path.exists(checkpoint_filepath):\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "else:\n",
    "    print(f\"Warning: Checkpoint file {checkpoint_filepath} not found. Using last model weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Avaliação do Modelo ---\n",
    "print(\"\\nPhase 5: Evaluating the Model on Test Set\")\n",
    "\n",
    "if test_samples_effective == 0:\n",
    "    print(\"ERROR: Test dataset is empty. Cannot evaluate model.\")\n",
    "    test_accuracy = 0\n",
    "    test_auc = 0\n",
    "else:\n",
    "    results = model.evaluate(test_dataset, verbose=1)\n",
    "    if len(results) >= 5: \n",
    "        final_loss = results[0]\n",
    "        final_acc = results[1]  # Accuracy\n",
    "        final_auc = results[2]  # AUC\n",
    "        final_precision = results[3]  # Precision\n",
    "        final_recall = results[4]  # Recall\n",
    "        print(f\"Final Loss: {final_loss}\")\n",
    "        print(f\"Final Accuracy: {final_acc}\")\n",
    "        print(f\"Final AUC: {final_auc}\")\n",
    "        print(f\"Final Precision: {final_precision}\")\n",
    "        print(f\"Final Recall: {final_recall}\")\n",
    "        history_plot_filename = f\"training_history_Acc{final_acc:.3f}_AUC{final_auc:.3f}.png\"\n",
    "    else:\n",
    "        print(\"Error: Not enough metrics returned from model.evaluate\")\n",
    "\n",
    "    print(\"Number of results:\", len(results))\n",
    "    print(\"Results:\", results)\n",
    "    y_pred_proba = model.predict(test_dataset)\n",
    "\n",
    "    y_true_test = []\n",
    "    for _, labels_batch in test_dataset.unbatch().batch(BATCH_SIZE): \n",
    "        y_true_test.extend(labels_batch.numpy())\n",
    "    y_true_test = np.array(y_true_test)\n",
    "\n",
    "    if len(target_names) <= 2: # Binary classification\n",
    "        y_pred_classes = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "    else: # Multiclass classification\n",
    "        y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "\n",
    "    if len(y_true_test) == 0:\n",
    "        print(\"Warning: No true labels extracted from the test set. Cannot generate classification report or confusion matrix.\")\n",
    "    elif len(y_true_test) != len(y_pred_classes):\n",
    "        print(f\"Warning: Mismatch in number of true labels ({len(y_true_test)}) and predicted classes ({len(y_pred_classes)}). Skipping report/matrix.\")\n",
    "    else:\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        print(classification_report(y_true_test, y_pred_classes, target_names=target_names, labels=range(len(target_names))))\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(y_true_test, y_pred_classes, labels=range(len(target_names)))\n",
    "        print(\"\\nConfusion Matrix (Test Set):\")\n",
    "        print(cm)\n",
    "\n",
    "        plt.figure(figsize=(8,8))\n",
    "        ax = plt.gca()\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "        disp.plot(cmap=plt.cm.Blues, ax=ax, values_format='d')\n",
    "        plt.title('Confusion Matrix (Test Set)', fontsize=18)\n",
    "        \n",
    "    \n",
    "        cm_save_path = os.path.join(OUTPUT_DIR, \"confusion_matrix.png\")\n",
    "        plt.savefig(cm_save_path)\n",
    "        print(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot Training History\n",
    "acc, val_acc, loss_hist, val_loss_hist, auc, val_auc = [], [], [], [], [], []\n",
    "epochs_range_head_len = 0\n",
    "epochs_range_total = range(len(acc))\n",
    "\n",
    "if epochs_range_total: # Only plot if there's history\n",
    "    plt.figure(figsize=(20, 8)) \n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(epochs_range_total, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range_total, val_acc, label='Validation Accuracy')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len -1 , color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs_range_total, loss_hist, label='Training Loss')\n",
    "    plt.plot(epochs_range_total, val_loss_hist, label='Validation Loss')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len -1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    # Plot AUC\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs_range_total, auc, label='Training AUC')\n",
    "    plt.plot(epochs_range_total, val_auc, label='Validation AUC')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation AUC')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('AUC')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    print(\"Available metrics names:\", model.metrics_names)\n",
    "    final_acc = results[1]  # Accuracy\n",
    "    final_auc = results[2]  # AUC\n",
    "    history_plot_filename = f\"training_history_Acc{final_acc:.3f}_AUC{final_auc:.3f}.png\"\n",
    "    history_save_path = os.path.join(OUTPUT_DIR, history_plot_filename)\n",
    "    plt.savefig(history_save_path)\n",
    "    print(f\"Training history plot saved to {history_save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history found to plot.\")\n",
    "\n",
    "print(\"\\n--- End of Training ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
