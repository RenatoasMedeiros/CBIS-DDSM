{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4086d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0 # Using a smaller EfficientNet variant\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fb3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration & Constants ---\n",
    "BASE_DATASET_PATH = './k_CBIS-DDSM/'\n",
    "CALC_METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'calc_case(with_jpg_img).csv')\n",
    "MASS_METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'mass_case(with_jpg_img).csv')\n",
    "\n",
    "IMAGE_ROOT_DIR = BASE_DATASET_PATH\n",
    "ACTUAL_IMAGE_FILES_BASE_DIR = os.path.join(IMAGE_ROOT_DIR, 'jpg_img')\n",
    "\n",
    "# Column in CSV that conceptually should point to ROIs, even if paths are flawed\n",
    "CONCEPTUAL_ROI_COLUMN_NAME = 'jpg_ROI_img_path'\n",
    "PATHOLOGY_COLUMN_NAME = 'pathology'\n",
    "CASE_TYPE_COLUMN_NAME = 'case_type'\n",
    "\n",
    "\n",
    "# Model & Training Parameters\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224 # EfficientNetB0 default input size\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50 # Initial epochs for head training\n",
    "FINE_TUNE_EPOCHS = 20 # Additional epochs for fine-tuning\n",
    "LEARNING_RATE = 1e-4\n",
    "RANDOM_STATE = 42\n",
    "# --- End of Configuration & Constants ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Initial Path Configuration Debug ---\")\n",
    "print(f\"Current working directory (CWD): {os.getcwd()}\")\n",
    "print(f\"BASE_DATASET_PATH (relative from CWD as defined): {BASE_DATASET_PATH}\")\n",
    "print(f\"CALC_METADATA_CSV_PATH (relative from CWD as defined): {CALC_METADATA_CSV_PATH}\") # MODIFIED\n",
    "print(f\"MASS_METADATA_CSV_PATH (relative from CWD as defined): {MASS_METADATA_CSV_PATH}\")   # ADDED\n",
    "print(f\"IMAGE_ROOT_DIR (relative from CWD as defined): {IMAGE_ROOT_DIR}\")\n",
    "print(f\"ACTUAL_IMAGE_FILES_BASE_DIR (relative from CWD as defined): {ACTUAL_IMAGE_FILES_BASE_DIR}\")\n",
    "\n",
    "# Resolve to absolute paths for clarity and checking\n",
    "abs_base_dataset_path = os.path.abspath(BASE_DATASET_PATH)\n",
    "abs_calc_metadata_csv_path = os.path.abspath(CALC_METADATA_CSV_PATH) # MODIFIED\n",
    "abs_mass_metadata_csv_path = os.path.abspath(MASS_METADATA_CSV_PATH)   # ADDED\n",
    "abs_image_root_dir = os.path.abspath(IMAGE_ROOT_DIR)\n",
    "abs_actual_image_files_base_dir = os.path.abspath(ACTUAL_IMAGE_FILES_BASE_DIR)\n",
    "\n",
    "print(f\"\\nResolved BASE_DATASET_PATH to absolute: {abs_base_dataset_path}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_base_dataset_path)} | Is Dir? {os.path.isdir(abs_base_dataset_path)}\")\n",
    "\n",
    "print(f\"Resolved CALC_METADATA_CSV_PATH to absolute: {abs_calc_metadata_csv_path}\") # MODIFIED\n",
    "print(f\"  -> Exists? {os.path.exists(abs_calc_metadata_csv_path)} | Is File? {os.path.isfile(abs_calc_metadata_csv_path)}\")\n",
    "\n",
    "print(f\"Resolved MASS_METADATA_CSV_PATH to absolute: {abs_mass_metadata_csv_path}\")   # ADDED\n",
    "print(f\"  -> Exists? {os.path.exists(abs_mass_metadata_csv_path)} | Is File? {os.path.isfile(abs_mass_metadata_csv_path)}\")\n",
    "\n",
    "print(f\"Resolved IMAGE_ROOT_DIR to absolute: {abs_image_root_dir}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_image_root_dir)} | Is Dir? {os.path.isdir(abs_image_root_dir)}\")\n",
    "\n",
    "print(f\"Resolved ACTUAL_IMAGE_FILES_BASE_DIR (where series folders should be): {abs_actual_image_files_base_dir}\")\n",
    "print(f\"  -> Exists? {os.path.exists(abs_actual_image_files_base_dir)} | Is Dir? {os.path.isdir(abs_actual_image_files_base_dir)}\")\n",
    "\n",
    "if os.path.exists(abs_actual_image_files_base_dir) and os.path.isdir(abs_actual_image_files_base_dir):\n",
    "    print(f\"\\nSample contents of ACTUAL_IMAGE_FILES_BASE_DIR ('{abs_actual_image_files_base_dir}') (first 10 items):\")\n",
    "    try:\n",
    "        sample_contents = os.listdir(abs_actual_image_files_base_dir)[:10]\n",
    "        if not sample_contents:\n",
    "            print(\"    -> Directory is empty or unreadable.\")\n",
    "        for item_idx, item in enumerate(sample_contents):\n",
    "            item_abs_path = os.path.join(abs_actual_image_files_base_dir, item)\n",
    "            item_type = \"Dir\" if os.path.isdir(item_abs_path) else \"File\" if os.path.isfile(item_abs_path) else \"Other\"\n",
    "            print(f\"    -> [{item_type}] {item}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Could not list directory contents: {e}\")\n",
    "else:\n",
    "    print(\"\\nCRITICAL WARNING: ACTUAL_IMAGE_FILES_BASE_DIR does not exist or is not a directory. Path searches will fail.\")\n",
    "print(\"--- End of Initial Path Configuration Debug ---\\n\")\n",
    "\n",
    "\n",
    "# MODIFIED: Load and combine Calc and Mass CSVs\n",
    "print(\"Proceeding with CSV loading...\")\n",
    "loaded_dfs = []\n",
    "\n",
    "# Load Calc cases\n",
    "if os.path.exists(abs_calc_metadata_csv_path):\n",
    "    try:\n",
    "        calc_df = pd.read_csv(abs_calc_metadata_csv_path)\n",
    "        calc_df[CASE_TYPE_COLUMN_NAME] = 'calc' # Add case type identifier\n",
    "        loaded_dfs.append(calc_df)\n",
    "        print(f\"Successfully loaded and tagged {len(calc_df)} rows from {CALC_METADATA_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the CALC CSV ({CALC_METADATA_CSV_PATH}): {e}\")\n",
    "else:\n",
    "    print(f\"WARNING: CALC CSV file not found at {abs_calc_metadata_csv_path}. Skipping.\")\n",
    "\n",
    "# Load Mass cases\n",
    "if os.path.exists(abs_mass_metadata_csv_path):\n",
    "    try:\n",
    "        mass_df = pd.read_csv(abs_mass_metadata_csv_path)\n",
    "        mass_df[CASE_TYPE_COLUMN_NAME] = 'mass' # Add case type identifier\n",
    "        loaded_dfs.append(mass_df)\n",
    "        print(f\"Successfully loaded and tagged {len(mass_df)} rows from {MASS_METADATA_CSV_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the MASS CSV ({MASS_METADATA_CSV_PATH}): {e}\")\n",
    "else:\n",
    "    print(f\"WARNING: MASS CSV file not found at {abs_mass_metadata_csv_path}. Skipping.\")\n",
    "\n",
    "if not loaded_dfs:\n",
    "    print(\"ERROR: No CSV files were loaded. Cannot proceed.\")\n",
    "    raise FileNotFoundError(\"Neither Calc nor Mass CSV files could be loaded. Check paths and file existence.\")\n",
    "\n",
    "source_df = pd.concat(loaded_dfs, ignore_index=True)\n",
    "print(f\"Combined DataFrame created with {len(source_df)} total rows from {len(loaded_dfs)} CSV file(s).\")\n",
    "print(f\"Columns available in combined DataFrame: {source_df.columns.tolist()}\")\n",
    "\n",
    "\n",
    "# Clean and filter initial dataframe\n",
    "if CONCEPTUAL_ROI_COLUMN_NAME not in source_df.columns or PATHOLOGY_COLUMN_NAME not in source_df.columns:\n",
    "    print(f\"ERROR: Required columns for metadata ('{CONCEPTUAL_ROI_COLUMN_NAME}' or '{PATHOLOGY_COLUMN_NAME}') not in combined CSV.\")\n",
    "    print(f\"Available columns are: {source_df.columns.tolist()}\")\n",
    "    raise KeyError(\"Missing essential columns in combined CSV.\")\n",
    "\n",
    "source_df.dropna(subset=[CONCEPTUAL_ROI_COLUMN_NAME, PATHOLOGY_COLUMN_NAME], inplace=True)\n",
    "source_df = source_df[source_df[PATHOLOGY_COLUMN_NAME].isin(['MALIGNANT', 'BENIGN'])]\n",
    "print(f\"Rows after initial cleaning (dropna on conceptual ROI/pathology, pathology filter): {len(source_df)}\")\n",
    "\n",
    "if source_df.empty:\n",
    "    raise ValueError(\"Combined DataFrame is empty after initial cleaning. Cannot proceed.\")\n",
    "\n",
    "def heuristic_find_image_path(row, actual_images_root_dir_abs):\n",
    "    try:\n",
    "        patient_id = row['patient_id']\n",
    "        breast_side = row['left or right breast']\n",
    "        image_view = row['image view']\n",
    "        abnormality_id = str(row['abnormality id']) # Ensure it's a string for concatenation\n",
    "\n",
    "        csv_conceptual_roi_path = str(row.get(CONCEPTUAL_ROI_COLUMN_NAME, \"\")).strip()\n",
    "\n",
    "        case_type_folder_prefix = \"\"\n",
    "        if csv_conceptual_roi_path.startswith(\"jpg_img/\"):\n",
    "            path_part = csv_conceptual_roi_path.split('/')[1] # e.g., \"Calc_Training_P_00005_...\" or \"Mass_Test_P_00001_...\"\n",
    "            # Extract the part before patient_id\n",
    "            # The heuristic already includes Mass_Training and Mass_Test\n",
    "            if path_part.startswith(\"Calc_Training_\"): case_type_folder_prefix = \"Calc_Training\"\n",
    "            elif path_part.startswith(\"Calc_Test_\"): case_type_folder_prefix = \"Calc_Test\"\n",
    "            elif path_part.startswith(\"Mass_Training_\"): case_type_folder_prefix = \"Mass_Training\"\n",
    "            elif path_part.startswith(\"Mass_Test_\"): case_type_folder_prefix = \"Mass_Test\"\n",
    "\n",
    "        if not case_type_folder_prefix:\n",
    "            # print(f\"DEBUG (heuristic): Could not determine case_type_folder_prefix for {patient_id} from '{csv_conceptual_roi_path}'\")\n",
    "            return None\n",
    "\n",
    "        # Form search pattern for directories: e.g., /path/to/jpg_img/Calc_Training_P_00005_RIGHT_CC_1-*\n",
    "        dir_search_prefix = f\"{case_type_folder_prefix}_{patient_id}_{breast_side}_{image_view}_{abnormality_id}\"\n",
    "        full_dir_search_pattern = os.path.join(actual_images_root_dir_abs, f\"{dir_search_prefix}-*\")\n",
    "\n",
    "        potential_series_dirs = glob.glob(full_dir_search_pattern)\n",
    "\n",
    "        if not potential_series_dirs:\n",
    "            # print(f\"DEBUG (heuristic): No series directory found for {patient_id} with pattern '{full_dir_search_pattern}'\")\n",
    "            return None\n",
    "\n",
    "        roi_filename_patterns = [\n",
    "            \"ROI-mask-images-img_0-*.jpg\", \"ROI-mask-images-img_1-*.jpg\", \"ROI-mask-images-img_*-*.jpg\"\n",
    "        ]\n",
    "\n",
    "        for series_dir_on_disk in sorted(potential_series_dirs): # Sort to get a consistent choice if multiple match\n",
    "            if os.path.isdir(series_dir_on_disk):\n",
    "                for pattern in roi_filename_patterns:\n",
    "                    image_search_glob = os.path.join(series_dir_on_disk, pattern)\n",
    "                    found_roi_files = glob.glob(image_search_glob)\n",
    "                    if found_roi_files:\n",
    "                        found_roi_files.sort() # Sort to get a consistent choice\n",
    "                        return found_roi_files[0] # Return the first valid ROI found\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # print(f\"DEBUG (heuristic): Error for row {row.get('patient_id', 'Unknown')} ({row.get(CASE_TYPE_COLUMN_NAME, 'N/A')} case): {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Attempting HEURISTIC search for valid ROI paths for each CSV entry...\")\n",
    "source_df['full_image_path'] = source_df.apply(\n",
    "    lambda r: heuristic_find_image_path(r, abs_actual_image_files_base_dir), axis=1\n",
    ")\n",
    "\n",
    "# All columns from source_df (including 'case_type' and any other original metadata)\n",
    "# will be carried into metadata_df for rows where an image path was found.\n",
    "metadata_df = source_df.dropna(subset=['full_image_path']).copy()\n",
    "found_image_count = len(metadata_df)\n",
    "print(f\"Found {found_image_count} actual image files (ROIs if available) after HEURISTIC search from combined data.\")\n",
    "print(f\"Breakdown by case type (if available in metadata_df): \\n{metadata_df[CASE_TYPE_COLUMN_NAME].value_counts()}\")\n",
    "\n",
    "\n",
    "if found_image_count == 0:\n",
    "    print(\"CRITICAL ERROR: Still no valid image files found even after heuristic search from combined data.\")\n",
    "    raise FileNotFoundError(\"No usable image files found even with heuristic search from combined data.\")\n",
    "\n",
    "metadata_df.rename(columns={'full_image_path': 'full_roi_path'}, inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# Ensure 'pathology_encoded' is created correctly on the copied DataFrame slice\n",
    "metadata_df.loc[:, 'pathology_encoded'] = label_encoder.fit_transform(metadata_df[PATHOLOGY_COLUMN_NAME])\n",
    "target_names = list(label_encoder.classes_)\n",
    "\n",
    "# X will contain the image paths, y will contain the encoded labels.\n",
    "# All other metadata columns (like 'patient_id', 'case_type', etc.) remain in metadata_df\n",
    "# and can be used for deeper analysis or if a multi-input model is developed later.\n",
    "X = metadata_df['full_roi_path']\n",
    "y = metadata_df['pathology_encoded']\n",
    "print(f\"Number of samples going into train_test_split: {len(X)}\")\n",
    "\n",
    "if len(X) == 0:\n",
    "     raise ValueError(\"Dataset is empty, cannot split.\")\n",
    "\n",
    "# Stratify by y to ensure balanced splits, especially important if classes are imbalanced\n",
    "# or if combining datasets leads to different proportions.\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, random_state=RANDOM_STATE, stratify=y_train_val # Stratify this split too\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Print class distribution in each set to verify stratification\n",
    "print(f\"Train labels distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Validation labels distribution: {np.bincount(y_val)}\")\n",
    "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path_tensor, label_tensor):\n",
    "    # This function is wrapped by tf.py_function, so inputs are tensors.\n",
    "    # Convert image_path_tensor to a Python string for cv2.imread\n",
    "    image_path_str = image_path_tensor.numpy().decode('utf-8')\n",
    "\n",
    "    try:\n",
    "        img = cv2.imread(image_path_str)\n",
    "        if img is None: # Check if image loading failed\n",
    "            # print(f\"Warning: Could not read image {image_path_str}. Returning dummy data.\")\n",
    "            dummy_img_array = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.float32)\n",
    "            dummy_img_processed = tf.keras.applications.efficientnet.preprocess_input(dummy_img_array.copy())\n",
    "            error_label = np.int32(-1) # Special label to indicate a problem\n",
    "            return dummy_img_processed, error_label\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # OpenCV loads as BGR, convert to RGB\n",
    "        img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        img_float32 = img.astype(np.float32)\n",
    "        img_processed = tf.keras.applications.efficientnet.preprocess_input(img_float32)\n",
    "\n",
    "        return img_processed, label_tensor.numpy().astype(np.int32)\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error processing image {image_path_str}: {e}. Returning dummy data.\")\n",
    "        dummy_img_array = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.float32)\n",
    "        dummy_img_processed = tf.keras.applications.efficientnet.preprocess_input(dummy_img_array.copy())\n",
    "        error_label = np.int32(-1) # Special label\n",
    "        return dummy_img_processed, error_label\n",
    "\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomBrightness(factor=0.1),\n",
    "    tf.keras.layers.RandomContrast(factor=0.1)\n",
    "])\n",
    "\n",
    "def create_tf_dataset(image_paths, labels, batch_size, augment=False):\n",
    "    image_paths_list = list(image_paths)\n",
    "    labels_list = list(labels)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths_list, labels_list))\n",
    "\n",
    "    dataset = dataset.map(lambda x, y: tf.py_function(\n",
    "        load_and_preprocess_image,\n",
    "        [x, y],\n",
    "        [tf.float32, tf.int32]),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.filter(lambda img_tensor, label_tensor: label_tensor != -1)\n",
    "\n",
    "    def set_shape(img, label):\n",
    "        img.set_shape((IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "        label.set_shape(())\n",
    "        return img, label\n",
    "    dataset = dataset.map(set_shape, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    if augment:\n",
    "        dataset = dataset.map(lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "print(\"Recreating TensorFlow datasets with updated image loading logic...\")\n",
    "train_dataset = create_tf_dataset(X_train, y_train, BATCH_SIZE, augment=True)\n",
    "val_dataset = create_tf_dataset(X_val, y_val, BATCH_SIZE, augment=False)\n",
    "test_dataset = create_tf_dataset(X_test, y_test, BATCH_SIZE, augment=False)\n",
    "\n",
    "print(\"Verifying dataset integrity (this might take a moment)...\")\n",
    "train_batches = 0\n",
    "train_samples_effective = 0\n",
    "for images, labels in train_dataset:\n",
    "    train_batches += 1\n",
    "    train_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in train_dataset: {train_batches}\")\n",
    "print(f\"Effective number of samples in train_dataset after filtering: {train_samples_effective}\")\n",
    "\n",
    "if train_batches > 0:\n",
    "    for images, labels in train_dataset.take(1):\n",
    "        print(\"Sample batch shape from train_dataset:\", images.shape, labels.shape)\n",
    "else:\n",
    "    print(\"Warning: train_dataset is empty after filtering. Check for widespread image loading issues.\")\n",
    "\n",
    "val_batches = 0\n",
    "val_samples_effective = 0\n",
    "for images, labels in val_dataset:\n",
    "    val_batches +=1\n",
    "    val_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in val_dataset: {val_batches}\")\n",
    "print(f\"Effective number of samples in val_dataset after filtering: {val_samples_effective}\")\n",
    "\n",
    "\n",
    "test_batches = 0\n",
    "test_samples_effective = 0\n",
    "for images, labels in test_dataset:\n",
    "    test_batches += 1\n",
    "    test_samples_effective += labels.shape[0]\n",
    "print(f\"Number of batches in test_dataset: {test_batches}\")\n",
    "print(f\"Effective number of samples in test_dataset after filtering: {test_samples_effective}\")\n",
    "\n",
    "\n",
    "# Check if any dataset is empty, which could cause issues during training/evaluation\n",
    "if train_samples_effective == 0 or val_samples_effective == 0:\n",
    "    print(\"CRITICAL WARNING: Training or Validation dataset is empty after processing. Model training cannot proceed effectively.\")\n",
    "    # Depending on the severity, you might want to raise an error here\n",
    "    # raise ValueError(\"Training or Validation dataset is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c15a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Model Architecture ---\n",
    "print(\"\\nPhase 2: Building the Model\")\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet',\n",
    "                            input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "base_model.trainable = False # Start with base model frozen\n",
    "\n",
    "inputs = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D(name=\"avg_pool\")(x)\n",
    "x = Dropout(0.3, name=\"top_dropout_1\")(x)\n",
    "x = Dense(128, activation='relu', name=\"dense_128\")(x)\n",
    "x = Dropout(0.3, name=\"top_dropout_2\")(x)\n",
    "outputs = Dense(1, activation='sigmoid', name=\"predictions\")(x)\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acd817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Model Compilation ---\n",
    "print(\"\\nPhase 3: Compiling the Model\")\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# MODIFIED: Adjust loss based on number of classes\n",
    "if len(target_names) <= 2: # Binary classification (or single class if an error, but usually benign/malignant)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy()\n",
    "    # For binary, ensure 'accuracy' is suitable. AUC, Precision, Recall are fine.\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "               tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')]\n",
    "else: # Multiclass classification\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy() # Assuming y_train, etc., are integer labels\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name='auc')] # AUC might need `multi_label=True` or specific setup for multiclass\n",
    "    # For multiclass, typical metrics are accuracy, sparse_categorical_accuracy.\n",
    "    # Precision and Recall can be more complex (e.g., weighted, macro).\n",
    "    # For simplicity, starting with accuracy and AUC.\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ce229",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPhase 4: Training the Model (Head Only)\")\n",
    "# Ensure datasets are not empty before starting training\n",
    "if train_samples_effective == 0 or val_samples_effective == 0:\n",
    "    print(\"ERROR: Cannot start head training because train or validation dataset is empty.\")\n",
    "else:\n",
    "    checkpoint_filepath_head = './best_model_head_only.keras'\n",
    "    callbacks_head = [\n",
    "        ModelCheckpoint(filepath=checkpoint_filepath_head, save_weights_only=False, monitor='val_auc', mode='max', save_best_only=True),\n",
    "        EarlyStopping(monitor='val_auc', patience=10, mode='max', restore_best_weights=True), # Increased patience\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7, mode='min') # Increased patience\n",
    "    ]\n",
    "    history_head = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks_head\n",
    "    )\n",
    "    print(\"Loading best weights from head training...\")\n",
    "    if os.path.exists(checkpoint_filepath_head):\n",
    "        model.load_weights(checkpoint_filepath_head)\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint file {checkpoint_filepath_head} not found. Using last model weights.\")\n",
    "\n",
    "\n",
    "# --- 4b. Fine-tuning Phase ---\n",
    "print(\"\\nPhase 4b: Fine-tuning (Unfreezing some base model layers)\")\n",
    "base_model.trainable = True\n",
    "\n",
    "# Unfreeze layers from a certain block onwards in EfficientNetB0\n",
    "# EfficientNetB0 has blocks named 'block2a_expand_conv', 'block3a_expand_conv', ..., 'block7a_expand_conv'\n",
    "# We can choose to unfreeze from 'block6a' or 'block5a' onwards\n",
    "# For this example, let's unfreeze from 'block5a' onwards.\n",
    "# You might need to inspect `base_model.summary()` to choose the right layers.\n",
    "\n",
    "# Fine-tuning strategy: Unfreeze more layers\n",
    "# Set base_model.trainable = True first\n",
    "# Then, selectively re-freeze earlier layers if desired\n",
    "# For EfficientNet, it's common to unfreeze the top blocks.\n",
    "\n",
    "fine_tune_at_layer_name = 'block6a_expand_conv' # Here we are just unfrezzing one\n",
    "set_trainable = False\n",
    "for layer in base_model.layers:\n",
    "    if layer.name == fine_tune_at_layer_name:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization): # Keep BN frozen\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False # Explicitly keep BN frozen\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "optimizer_fine_tune = Adam(learning_rate=LEARNING_RATE / 10) # Use a smaller LR\n",
    "model.compile(optimizer=optimizer_fine_tune, loss=loss, metrics=metrics) # Re-compile\n",
    "model.summary() # Show summary with new trainable params\n",
    "\n",
    "if train_samples_effective == 0 or val_samples_effective == 0:\n",
    "     print(\"ERROR: Cannot start fine-tuning because train or validation dataset is empty.\")\n",
    "else:\n",
    "    checkpoint_filepath_finetune = './best_model_finetuned.keras'\n",
    "    callbacks_finetune = [\n",
    "        ModelCheckpoint(filepath=checkpoint_filepath_finetune, save_weights_only=False, monitor='val_auc', mode='max', save_best_only=True),\n",
    "        EarlyStopping(monitor='val_auc', patience=15, mode='max', restore_best_weights=True), # Increased patience for fine-tuning\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-8, mode='min') # Increased patience\n",
    "    ]\n",
    "\n",
    "    # Determine initial epoch for fine-tuning\n",
    "    initial_fine_tune_epoch = 0\n",
    "    if 'history_head' in locals() and hasattr(history_head, 'epoch') and history_head.epoch:\n",
    "        initial_fine_tune_epoch = history_head.epoch[-1] + 1\n",
    "    else: # If head training was skipped or history is unavailable\n",
    "        initial_fine_tune_epoch = 0 # Or EPOCHS if you want to assume head training ran for all its epochs\n",
    "        EPOCHS = 0 # Ensure we don't re-run head training if it was skipped\n",
    "\n",
    "    history_fine_tune = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS + FINE_TUNE_EPOCHS, # Total epochs\n",
    "        initial_epoch=initial_fine_tune_epoch, # Continue from where head training left off\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=callbacks_finetune\n",
    "    )\n",
    "    print(\"Loading best weights from fine-tuning...\")\n",
    "    if os.path.exists(checkpoint_filepath_finetune):\n",
    "        model.load_weights(checkpoint_filepath_finetune)\n",
    "    else:\n",
    "        print(f\"Warning: Checkpoint file {checkpoint_filepath_finetune} not found. Using last model weights from fine-tuning.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Model Evaluation ---\n",
    "print(\"\\nPhase 5: Evaluating the Model on Test Set\")\n",
    "\n",
    "if test_samples_effective == 0:\n",
    "    print(\"ERROR: Test dataset is empty. Cannot evaluate model.\")\n",
    "else:\n",
    "    results = model.evaluate(test_dataset, verbose=1)\n",
    "    print(\"\\nTest Set Evaluation Results:\")\n",
    "    if results and hasattr(model, 'metrics_names'):\n",
    "        for name, value in zip(model.metrics_names, results):\n",
    "            print(f\"{name}: {value:.4f}\")\n",
    "    else:\n",
    "        print(\"Evaluation did not return results or model has no metrics_names.\")\n",
    "\n",
    "    y_pred_proba = model.predict(test_dataset)\n",
    "\n",
    "    # Extract true labels correctly, regardless of whether test_dataset was batched\n",
    "    y_true_test = []\n",
    "    for _, labels_batch in test_dataset.unbatch().batch(BATCH_SIZE): # Re-batch after unbatching to iterate easily\n",
    "        y_true_test.extend(labels_batch.numpy())\n",
    "    y_true_test = np.array(y_true_test)\n",
    "\n",
    "    if len(target_names) <= 2: # Binary classification\n",
    "        y_pred_classes = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "    else: # Multiclass classification\n",
    "        y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "\n",
    "    if len(y_true_test) == 0:\n",
    "        print(\"Warning: No true labels extracted from the test set. Cannot generate classification report or confusion matrix.\")\n",
    "    elif len(y_true_test) != len(y_pred_classes):\n",
    "         print(f\"Warning: Mismatch in number of true labels ({len(y_true_test)}) and predicted classes ({len(y_pred_classes)}). Skipping report/matrix.\")\n",
    "    else:\n",
    "        print(\"\\nClassification Report (Test Set):\")\n",
    "        # Ensure target_names are correctly mapped if using numerical labels for the report\n",
    "        # For binary, target_names = ['BENIGN', 'MALIGNANT'] (or whatever label_encoder found)\n",
    "        # For multiclass, ensure it matches the numerically encoded classes\n",
    "        print(classification_report(y_true_test, y_pred_classes, target_names=target_names, labels=range(len(target_names))))\n",
    "\n",
    "\n",
    "        cm = confusion_matrix(y_true_test, y_pred_classes, labels=range(len(target_names)))\n",
    "        print(\"\\nConfusion Matrix (Test Set):\")\n",
    "        print(cm)\n",
    "\n",
    "        plt.figure(figsize=(6,6))\n",
    "        ax = plt.gca()\n",
    "        ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(x=j, y=i, s=cm[i, j], va='center', ha='center', size='xx-large')\n",
    "        plt.xlabel('Predictions', fontsize=18)\n",
    "        plt.ylabel('Actuals', fontsize=18)\n",
    "        plt.title('Confusion Matrix (Test Set)', fontsize=18)\n",
    "        ax.set_xticks(range(len(target_names)))\n",
    "        ax.set_yticks(range(len(target_names)))\n",
    "        ax.set_xticklabels(target_names)\n",
    "        ax.set_yticklabels(target_names)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Plot Training History (combined)\n",
    "# Ensure histories exist before trying to plot\n",
    "acc, val_acc, loss_hist, val_loss_hist, auc, val_auc = [], [], [], [], [], []\n",
    "epochs_range_head_len = 0\n",
    "\n",
    "if 'history_head' in locals() and hasattr(history_head, 'history'):\n",
    "    acc.extend(history_head.history.get('accuracy', []))\n",
    "    val_acc.extend(history_head.history.get('val_accuracy', []))\n",
    "    loss_hist.extend(history_head.history.get('loss', []))\n",
    "    val_loss_hist.extend(history_head.history.get('val_loss', []))\n",
    "    auc.extend(history_head.history.get('auc', []))\n",
    "    val_auc.extend(history_head.history.get('val_auc', []))\n",
    "    epochs_range_head_len = len(history_head.history.get('accuracy', []))\n",
    "\n",
    "\n",
    "if 'history_fine_tune' in locals() and hasattr(history_fine_tune, 'history'):\n",
    "    acc.extend(history_fine_tune.history.get('accuracy', []))\n",
    "    val_acc.extend(history_fine_tune.history.get('val_accuracy', []))\n",
    "    loss_hist.extend(history_fine_tune.history.get('loss', []))\n",
    "    val_loss_hist.extend(history_fine_tune.history.get('val_loss', []))\n",
    "    auc.extend(history_fine_tune.history.get('auc', []))\n",
    "    val_auc.extend(history_fine_tune.history.get('val_auc', []))\n",
    "\n",
    "epochs_range_total = range(len(acc))\n",
    "\n",
    "if epochs_range_total: # Only plot if there's history\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(epochs_range_total, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range_total, val_acc, label='Validation Accuracy')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len -1 , color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(epochs_range_total, loss_hist, label='Training Loss')\n",
    "    plt.plot(epochs_range_total, val_loss_hist, label='Validation Loss')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len -1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(epochs_range_total, auc, label='Training AUC')\n",
    "    plt.plot(epochs_range_total, val_auc, label='Validation AUC')\n",
    "    if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "        plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation AUC')\n",
    "\n",
    "    # Add precision and recall plots if desired and available\n",
    "    precision, val_precision, recall, val_recall = [], [], [], []\n",
    "    if 'history_head' in locals() and hasattr(history_head, 'history'):\n",
    "        precision.extend(history_head.history.get('precision', []))\n",
    "        val_precision.extend(history_head.history.get('val_precision', []))\n",
    "        recall.extend(history_head.history.get('recall', []))\n",
    "        val_recall.extend(history_head.history.get('val_recall', []))\n",
    "    if 'history_fine_tune' in locals() and hasattr(history_fine_tune, 'history'):\n",
    "        precision.extend(history_fine_tune.history.get('precision', []))\n",
    "        val_precision.extend(history_fine_tune.history.get('val_precision', []))\n",
    "        recall.extend(history_fine_tune.history.get('recall', []))\n",
    "        val_recall.extend(history_fine_tune.history.get('val_recall', []))\n",
    "\n",
    "\n",
    "    if precision: # Check if precision was recorded\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.plot(epochs_range_total, precision, label='Training Precision')\n",
    "        plt.plot(epochs_range_total, val_precision, label='Validation Precision')\n",
    "        if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "            plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('Training and Validation Precision')\n",
    "\n",
    "    if recall: # Check if recall was recorded\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.plot(epochs_range_total, recall, label='Training Recall')\n",
    "        plt.plot(epochs_range_total, val_recall, label='Validation Recall')\n",
    "        if epochs_range_head_len > 0 and epochs_range_head_len < len(epochs_range_total):\n",
    "            plt.axvline(x=epochs_range_head_len-1, color='gray', linestyle='--', label='Start Fine-tuning')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('Training and Validation Recall')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history found to plot.\")\n",
    "\n",
    "print(\"\\n--- End of Training ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
