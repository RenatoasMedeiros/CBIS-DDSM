{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7665a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from datetime import datetime\n",
    "import tensorflow_hub as hub\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration & Constants\n",
    "# ---------------------------\n",
    "\n",
    "MODEL_NAME = 'FasterRCNN_BreastCancer_OD'\n",
    "BASE_DATASET_PATH = './k_CBIS-DDSM/'\n",
    "CALC_METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'calc_case(with_jpg_img).csv')\n",
    "MASS_METADATA_CSV_PATH = os.path.join(BASE_DATASET_PATH, 'mass_case(with_jpg_img).csv')\n",
    "ACTUAL_IMAGE_FILES_BASE_DIR = os.path.join(BASE_DATASET_PATH, 'jpg_img')\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "OUTPUT_DIR = os.path.join('./', f\"run_{MODEL_NAME}_{IMG_WIDTH}_{BATCH_SIZE}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Assume each row in CSV has bounding box info: x_min, y_min, x_max, y_max\n",
    "# Add those columns to your CSVs if not already present\n",
    "BOX_COLUMNS = ['x_min', 'y_min', 'x_max', 'y_max']\n",
    "CLASS_COLUMN = 'pathology'\n",
    "\n",
    "# Label map\n",
    "LABEL_MAP = {\"BENIGN\": 1, \"MALIGNANT\": 2}\n",
    "NUM_CLASSES = len(LABEL_MAP)  # Background is class 0\n",
    "\n",
    "# ---------------------------\n",
    "# Load and Preprocess Data\n",
    "# ---------------------------\n",
    "\n",
    "def load_metadata():\n",
    "    df_list = []\n",
    "    for csv_path in [CALC_METADATA_CSV_PATH, MASS_METADATA_CSV_PATH]:\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path)\n",
    "            df['case_type'] = os.path.basename(csv_path).split('_')[0].lower()\n",
    "            df_list.append(df)\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def heuristic_find_image_path(row, base_dir):\n",
    "    # Same logic as before to find actual image path\n",
    "    pass  # You already have this function from your original code\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = heuristic_find_image_path(row, ACTUAL_IMAGE_FILES_BASE_DIR)\n",
    "        if img_path is None:\n",
    "            continue\n",
    "        # Ensure bounding box values are available\n",
    "        if any(row[col] is None for col in BOX_COLUMNS):\n",
    "            continue\n",
    "        records.append({\n",
    "            'image_path': img_path,\n",
    "            'bboxes': [row[c] for c in BOX_COLUMNS],\n",
    "            'label': LABEL_MAP[row[CLASS_COLUMN]]\n",
    "        })\n",
    "    return records\n",
    "\n",
    "print(\"Loading metadata...\")\n",
    "metadata_df = load_metadata()\n",
    "metadata_df.dropna(subset=BOX_COLUMNS, inplace=True)\n",
    "dataset_records = prepare_dataset(metadata_df)\n",
    "\n",
    "train_records, test_records = train_test_split(dataset_records, test_size=0.2, random_state=42)\n",
    "train_records, val_records = train_test_split(train_records, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Total samples: {len(dataset_records)}\")\n",
    "print(f\"Train: {len(train_records)}, Val: {len(val_records)}, Test: {len(test_records)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Data Loading and Augmentation\n",
    "# ---------------------------\n",
    "\n",
    "def parse_example(record):\n",
    "    image = tf.io.read_file(record['image_path'])\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    image /= 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    bboxes = tf.constant(record['bboxes'], dtype=tf.float32)\n",
    "    bboxes /= tf.constant([image.shape[1], image.shape[0], image.shape[1], image.shape[0]], dtype=tf.float32)\n",
    "    classes = tf.constant([record['label']], dtype=tf.int64)\n",
    "\n",
    "    return image, {\n",
    "        'boxes': bboxes[tf.newaxis, :],\n",
    "        'classes': classes\n",
    "    }\n",
    "\n",
    "def create_dataset(records):\n",
    "    def gen():\n",
    "        for r in records:\n",
    "            yield parse_example(r)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float32, {'boxes': tf.float32, 'classes': tf.int64}),\n",
    "        output_shapes=(\n",
    "            (IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "            {'boxes': (1, 4), 'classes': (1,)}\n",
    "        )\n",
    "    )\n",
    "    return dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = create_dataset(train_records)\n",
    "val_dataset = create_dataset(val_records)\n",
    "test_dataset = create_dataset(test_records)\n",
    "\n",
    "# ---------------------------\n",
    "# Load Pretrained Detection Model\n",
    "# ---------------------------\n",
    "\n",
    "print(\"Loading Faster R-CNN model from TF Hub...\")\n",
    "hub_url = \"https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1\" \n",
    "model = hub.load(hub_url)\n",
    "\n",
    "class DetectionModel(Model):\n",
    "    def __init__(self, model_url, num_classes, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model = hub.load(model_url)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        return self.model(inputs)\n",
    "\n",
    "detection_model = DetectionModel(hub_url, NUM_CLASSES + 1)  # + background\n",
    "\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "# ---------------------------\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "@tf.function\n",
    "def train_step(images, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = detection_model(images, training=True)\n",
    "        loss_dict = outputs['losses']\n",
    "        total_loss = sum(loss_dict.values())\n",
    "    gradients = tape.gradient(total_loss, detection_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, detection_model.trainable_variables))\n",
    "    return loss_dict\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    for step, (images, targets) in enumerate(train_dataset):\n",
    "        loss_dict = train_step(images, targets)\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step}: Losses: {loss_dict}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluation on Test Set\n",
    "# ---------------------------\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    all_detections = []\n",
    "    for images, _ in dataset:\n",
    "        outputs = model(images, training=False)\n",
    "        all_detections.extend(outputs)\n",
    "    return all_detections\n",
    "\n",
    "print(\"\\nEvaluating model...\")\n",
    "detections = evaluate(detection_model, test_dataset)\n",
    "\n",
    "# Save model\n",
    "model_save_path = os.path.join(OUTPUT_DIR, \"faster_rcnn_breast_cancer_od.keras\")\n",
    "tf.saved_model.save(detection_model, model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
